{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "GhNC8yFuD2eo",
    "outputId": "6c34c190-b472-432a-db9f-8651150a56c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# % cd drive/My Drive/hw3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAbI82HvEJgL"
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "import ranking as rnk\n",
    "import evaluate as evl\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import itertools\n",
    "\n",
    "import math\n",
    "data = dataset.get_dataset().get_data_folds()[0]\n",
    "data.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ILMI8Kmhakp5",
    "outputId": "15b00da0-bf30-408b-d7cd-520f671f90f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "2\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catlinbruys/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8339711885799135\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 0}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.836585835033126\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 1}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8385919713570898\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 2}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8405189486595361\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 3}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8423262520359234\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 4}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8431964399239792\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 5}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.842429820681789\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 150\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8346373574311795\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8379169071559202\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.839560917491348\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.841658754602168\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.842593420530521\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8425053520134533\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 200\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8348823937597747\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8385156597768338\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8403316008527147\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8414054098788085\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8427721828215904\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8429995427268225\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.843686939737706\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 200, 'epoch': 6}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.84322378989681\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 250\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8347639861926959\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.837596879375698\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8403020444045018\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8430016700647529\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8436649639119406\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8441688175761333\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 250, 'epoch': 5}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8446793121078117\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 250, 'epoch': 6}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8439419758345341\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 300\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8347100888604633\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8364143379868648\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8394374041285398\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8419264853870944\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.842976593002473\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.842392359461269\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 350\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8340079654679335\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8371583274610394\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8404642806785105\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8422296521447025\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8426169453203568\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8436767104692171\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8432021956807019\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 400\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8343115730783263\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8375269073250838\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8408233892758707\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8425459604936999\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8428539883090118\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.843837807434872\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.844326491391861\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8435490100788466\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 100\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8307926371745985\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.833376575643966\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8354184524475134\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8367972217958038\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8375555999810267\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8385389902292847\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8394543007109093\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.839927358575047\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8405471899012605\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8409151475213248\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 150\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8311202248257596\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8343033836401927\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8359966429962028\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8371668908483539\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.83767774978644\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8384240076789754\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8393579112677965\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8397636414379221\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8399791332027412\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8404971607252871\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 200\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8302748679221882\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8343986505469968\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.836021037883864\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8369971426245959\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8377841699966555\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8381855218287358\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8394066866276956\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.83951848195431\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.840108902909754\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8402987965026691\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 250\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8308315694499909\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8339376210005672\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8353947951773002\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.836970412545194\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8379696340901845\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8387448134683301\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8393283383153818\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8397231157916496\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8404736119765827\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8407443975308757\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 300\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8303032280546931\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8341768064068142\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8359883628745792\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8374871056985943\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8389153564466377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8399213375092767\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8403784165518466\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8408707925123406\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8409646609188093\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.841470012972615\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 350\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.83054144617121\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8339794009094857\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8358795990295078\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8370064568651575\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8381256182533403\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8388627432079988\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8392070080438895\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.840050919496826\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8404151209484553\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8406897778958511\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 400\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.830702230941838\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8340554460033924\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8356865110431451\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8365728965900796\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8374382415378103\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8388253971098176\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8398683555854377\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8395919549249498\n",
      "\n",
      "Testing learning_rate = 0.001 and n_hidden = 100\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8072995943053911\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8147601323383797\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8197762693262985\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8242850592471659\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.827718483272833\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8295811210143232\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8309310789400465\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8317155501896258\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.832791519153021\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8331764623342065\n",
      "\n",
      "Testing learning_rate = 0.001 and n_hidden = 150\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.807370709006693\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8159107170381433\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8218650808459238\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8256243948559079\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8288046379600104\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.831022987986223\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8320280480614146\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8332834559933222\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8339143480897572\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8346871877818697\n",
      "\n",
      "Testing learning_rate = 0.001 and n_hidden = 200\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8113457305409282\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8170776641395988\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8221706529564544\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8255708962142568\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8293639343267335\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8303807745139588\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8320432328224608\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8331464985887933\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.834249771230882\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.834490286605285\n",
      "\n",
      "Testing learning_rate = 0.001 and n_hidden = 250\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8119189596169405\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.818210142101592\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8233760561929394\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8271428733213249\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.829086709349755\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8306250667819793\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8320745417242632\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8335880416419649\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8343973706653393\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8346796057066908\n",
      "\n",
      "Testing learning_rate = 0.001 and n_hidden = 300\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8117474583909675\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8176600973059567\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8218054832335533\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8257636409727817\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8287803875875606\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8306739018808765\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8320833636950591\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8333611728108751\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.834730667222389\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8357016441036597\n",
      "\n",
      "Testing learning_rate = 0.001 and n_hidden = 350\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8105183335782565\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8169500737248464\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8221219257955691\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8259900184456724\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8293144791430285\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8314469151116138\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8323673133891735\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8335859071745035\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8346144785507457\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8353921037704096\n",
      "\n",
      "Testing learning_rate = 0.001 and n_hidden = 400\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8138156824912015\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.818550503726531\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.822740804077616\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8266357754444933\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8294042546339585\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8314174100478803\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8332393411815536\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8347280722942947\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8357817816728025\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8363895812615973\n",
      "\n",
      "Testing learning_rate = 0.0001 and n_hidden = 100\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.7629703821672839\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.7899433678832587\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.7979425335132457\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.802208022541284\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8052132776474805\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8068060453732862\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8074820356750534\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8086363415898304\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8092101235436067\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8099824737205816\n",
      "\n",
      "Testing learning_rate = 0.0001 and n_hidden = 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.7702173509077193\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.7892815564393412\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.7988967992001363\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8033685314099449\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8070413254837143\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8088839480261145\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8104182933309487\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8107388963825832\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8121125071572319\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8125482690084458\n",
      "\n",
      "Testing learning_rate = 0.0001 and n_hidden = 200\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.7928923825483036\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8039486983661346\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8089588914131227\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8108702685491082\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8116558038444904\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8123734020896746\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8133920830497502\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8136345365794426\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8140137668508447\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.814463326445308\n",
      "\n",
      "Testing learning_rate = 0.0001 and n_hidden = 250\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.7826780220332866\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.7954338940043827\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8001510313172254\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.80258231244661\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.804503482522137\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8058531315449504\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8066550780111552\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8077811228458345\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8081579653039492\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8089623538430492\n",
      "\n",
      "Testing learning_rate = 0.0001 and n_hidden = 300\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.754394592713793\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.7853360391500148\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.797847747802922\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8033329347346538\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8074659019676208\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8093446829529057\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8111844249641448\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8124674854872077\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8127623230509916\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8136920727786544\n",
      "\n",
      "Testing learning_rate = 0.0001 and n_hidden = 350\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.7784178396419934\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.7949361132994948\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8011813169724012\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8038511520903698\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8053905870517274\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8059999207258115\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8071114456368548\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8088890114966467\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8095814989923604\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.810353258435694\n",
      "\n",
      "Testing learning_rate = 0.0001 and n_hidden = 400\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.7802188988015051\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8032424846697673\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8084539585448741\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.810214770128164\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8111645395811518\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8116113834116828\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8126847924622438\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8129559234189553\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8134936980331287\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8141002906442714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'n_hidden': 250, 'epoch': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "class RankNet(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.output = torch.nn.Linear(n_hidden, 1)      \n",
    "        \n",
    "    def forward(self, x1):\n",
    "        x = torch.nn.functional.relu(self.hidden(x1))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, n_feature, n_hidden, learning_rate):\n",
    "        self.ranknet = RankNet(n_feature, n_hidden)\n",
    "        #.to(device)\n",
    "        self.optimizer = torch.optim.SGD(self.ranknet.parameters(), lr=learning_rate)\n",
    "\n",
    "def eval_model(model, data_fold):\n",
    "    #with torch.no_grad():\n",
    "        x = torch.from_numpy(data_fold.feature_matrix).float()\n",
    "        y = data_fold.label_vector\n",
    "        model.ranknet.eval()\n",
    "               \n",
    "        output = model.ranknet(x)\n",
    "        output = output.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        #loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-y)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
    "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    # data = dataset.get_dataset().get_data_folds()[0]\n",
    "    # data.read_data()\n",
    "\n",
    "    train_x = torch.from_numpy(data.train.feature_matrix).float()\n",
    "    train_y = torch.from_numpy(data.train.label_vector).float()\n",
    "\n",
    "    # documents = data.train.feature_matrix\n",
    "    # doc_list = list(range(len(documents)))\n",
    "    \n",
    "    # # Carthesian product\n",
    "    # Carth = list(itertools.combinations(doc_list,2))\n",
    "    # x1, x2, target = [], [], []\n",
    "  \n",
    "    # # iterate over all possible combinations\n",
    "    # for i,j in Carth:\n",
    "    #     x1.append(docs[i])\n",
    "    #     x2.append(docs[j])\n",
    "    #     if data.train.label_vector[i]>data.train.label_vector[j]:\n",
    "    #         # this is the S_{ij}\n",
    "    #         target.append(float(1))\n",
    "    #     elif data.train.label_vector[i]<data.train.label_vector[j]:\n",
    "    #         target.append(float(-1))\n",
    "    #     else:\n",
    "    #         target.append(float(0))\n",
    "\n",
    "\n",
    "    train_set = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "#     return torch.FloatTensor(x1), torch.FloatTensor(x2), torch.FloatTensor(target)\n",
    "    #return data\n",
    "    return train_loader\n",
    "       \n",
    "\n",
    "def plot_ndcg_loss(losses, ndcgs):\n",
    "    x = np.arange(len(losses))\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(x, losses, label='Loss')\n",
    "    ax.plot(x, ndcgs, label='NDCG')\n",
    "    ax.set_xlabel(\"Batch % 2000\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Pointwise LTR\")\n",
    "    legend = ax.legend(loc='upper center')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.savefig('Pointwise_LTR_plot.png')\n",
    "\n",
    "    \n",
    "def train_batch(documentfeatures, labels, model):\n",
    "#     model.ranknet.train()\n",
    "    for epoch in range(1):\n",
    "#         for qid in range(0, train_data.num_queries()):\n",
    "#             if train_data.query_size(qid) < 2:\n",
    "#                 continue\n",
    " \n",
    "            model.optimizer.zero_grad()\n",
    "\n",
    "            output = model.ranknet(documentfeatures)\n",
    "            \n",
    "            loss = pairwiseloss(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            model.optimizer.step()\n",
    "            \n",
    "    return model\n",
    "    \n",
    "def pairwiseloss(predictedvals, values):\n",
    "    \n",
    "    predictedvals = predictedvals.squeeze()\n",
    "\n",
    "    # Hier is waar er is gesleuteld aan de data om de nan's te voorkomen\n",
    "    if predictedvals.shape[0] == 0:\n",
    "        return torch.tensor([0.0], requires_grad= True)\n",
    "    # pairs = int(math.factorial(n_docs) / (math.factorial(n_docs - 2) * 2))\n",
    "    tups = list(itertools.combinations(range(predictedvals.shape[0]), 2))\n",
    "    \n",
    "\n",
    "    val1, val2 = [x[0] for x in tups], [x[1] for x in tups]\n",
    "    #todevice\n",
    "    pred1 = predictedvals[val1]\n",
    "    pred2 = predictedvals[val2]\n",
    "    \n",
    "    true1 = values[val1]\n",
    "    true2 = values[val2]\n",
    "    #.todevice\n",
    "    l1 = (true1 > true2).type(torch.ByteTensor)\n",
    "    l2 = (true1 < true2).type(torch.ByteTensor)\n",
    "    \n",
    "    S =  l1 - l2\n",
    "    S = torch.tensor(S)\n",
    "    sigmoid = torch.sigmoid(pred1.float() - pred2.float())\n",
    "    C_T = (0.5 * (1 - S) * sigmoid + torch.log(1 + torch.exp(-sigmoid)))\n",
    "    \n",
    "\n",
    "    return C_T.mean()\n",
    "\n",
    "def hyperparam_search():\n",
    "    # hyper-parameters\n",
    "    epochs = 10\n",
    "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
    "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
    "#     learning_rates = [0.001]\n",
    "#     n_hiddens = [100]\n",
    "    print(\"hi\")\n",
    "#     train_loader = load_dataset()\n",
    "    print(\"2\")\n",
    "    best_ndcg = 0\n",
    "    for learning_rate in learning_rates:\n",
    "        for n_hidden in n_hiddens:\n",
    "        \n",
    "            print(\"\\nTesting learning_rate = {} and n_hidden = {}\".format(learning_rate, n_hidden))\n",
    "            model = Model(data.num_features, n_hidden, learning_rate)\n",
    "            \n",
    "            last_ndcg = 0\n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                model.ranknet.train()\n",
    "                for qid in range(0, data.train.num_queries()):#\n",
    "                    if data.train.query_size(qid) < 2:#\n",
    "                        continue#\n",
    "                    s_i, e_i = data.train.query_range(qid)\n",
    "            \n",
    "                    documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
    "                    labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
    "#                     if documentfeatures.shape[0] == 0:\n",
    "#                         return torch.tensor([0.0], requires_grad= True)\n",
    "\n",
    "                    model = train_batch(documentfeatures, labels, model)  \n",
    "\n",
    "                                       \n",
    "                scores = eval_model(model, data.validation)\n",
    "              \n",
    "                ndcg = scores[\"ndcg\"][0]\n",
    "                print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
    "                            \n",
    "                if ndcg < last_ndcg:\n",
    "                    break\n",
    "                last_ndcg = ndcg\n",
    "                if ndcg > best_ndcg:\n",
    "                    best_ndcg = ndcg\n",
    "                    best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch}            \n",
    "                    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "hyperparam_search()\n",
    "\n",
    "\n",
    "\n",
    "#### Loss functie 1tje kiezen voor validation en train.\n",
    "####  \n",
    "\n",
    "# {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veVi9--TlXx6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_best(best_params):\n",
    "    epochs = best_params[\"epoch\"]\n",
    "    n_hidden = best_params[\"n_hidden\"]\n",
    "    learning_rate = best_params[\"learning_rate\"]\n",
    "    \n",
    "    # load data\n",
    "#     data, train_loader = load_dataset()\n",
    "    model = Model(data.num_features, n_hidden, learning_rate)\n",
    "\n",
    "    losses, ndcgs = [], []\n",
    "    for epoch in range(epochs):\n",
    "        eval_count = 0\n",
    "        for qid in range(0, data.train.num_queries()):#\n",
    "            if data.train.query_size(qid) < 2:#\n",
    "                continue#\n",
    "            s_i, e_i = data.train.query_range(qid)\n",
    "            \n",
    "            documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
    "            labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
    "            model = train_batch(documentfeatures, labels, model) \n",
    "            eval_count +=1\n",
    "            if eval_count % 2000 == 0:\n",
    "                loss, scores = eval_model(model, data.validation)\n",
    "                losses.append(loss)\n",
    "                ndcgs.append(scores[\"ndcg\"][0])\n",
    "        print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
    "        \n",
    "    return ndcgs, losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uy_km10Varfn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "2\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catlinbruys/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8342661932641088\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 0}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8380042037604613\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 1}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8397042115047316\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 2}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.841438670211334\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 3}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.841388114939132\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 150\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8348390950434817\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8376546130755324\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8406267082843206\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8414030421801957\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8430245667892994\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 4}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8436998278314941\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 5}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8433281038427017\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 200\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8347640507360036\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8374802156049957\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8403649919994496\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8413221639620887\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8421132425130146\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8441427592479028\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 200, 'epoch': 5}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.844213798817872\n",
      "Best parameters: {'learning_rate': 0.1, 'n_hidden': 200, 'epoch': 6}\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8441846990040772\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 250\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8346807683857161\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8380398750523249\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8403440349510453\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8419869795828403\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8419925332857326\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8431058975848513\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8436266013195969\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8431417299073348\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 300\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8348450842923801\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8374832533570677\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.839661105318633\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8413565846656365\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8423723716619737\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.842783526470207\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8433379144880989\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8435540724382461\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8430304471981593\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 350\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8342675515839475\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8374373367613683\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8405680392289091\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8423115295121729\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8427448436501328\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8429481153472801\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8426621017114146\n",
      "\n",
      "Testing learning_rate = 0.1 and n_hidden = 400\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8338939456452784\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8376547044796188\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8405237046440193\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8417425880563982\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8429822731736432\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8430115991189724\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8430325817173034\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8421477013055313\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 100\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8302976547182103\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8341238533929085\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8354504572696373\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.836673567726232\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8372430946394875\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8379036410404314\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8385919049802599\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8391790209651454\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8397469390768383\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8397776454720167\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 150\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8310388671799857\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8337417381476262\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8357961789846301\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8369225441875979\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8376143711537404\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8383077960224916\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8392780110450969\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8396262034096476\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8396865679868997\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8404897374827323\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 200\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8297684278024825\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8329901133687626\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8356062944432864\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8369275237074971\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8365759551315483\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 250\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8308528995850926\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8340150146741295\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8352443056535583\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8367904085730431\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8378134716442002\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8383853026368002\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8396169777713259\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8399224354598293\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.8407225648445199\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8410240049473984\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 300\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8311633111587422\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.835249903397772\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8363148247839665\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8369269361347792\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8380420258904597\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.839028037806919\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.8396989921015252\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 7, ndcg: 0.8403594958074829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 8, ndcg: 0.840719278074929\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 9, ndcg: 0.8412214656486979\n",
      "\n",
      "Testing learning_rate = 0.01 and n_hidden = 350\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 0, ndcg: 0.8318629954032453\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 1, ndcg: 0.8343491319352953\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 2, ndcg: 0.8359726556861994\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 3, ndcg: 0.8369390553993352\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 4, ndcg: 0.8376904487608666\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 5, ndcg: 0.8387170475682597\n",
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "Epoch: 6, ndcg: 0.839838594512447\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def train_best(best_params):\n",
    "#     epochs = best_params[\"epoch\"]\n",
    "#     n_hidden = best_params[\"n_hidden\"]\n",
    "#     learning_rate = best_params[\"learning_rate\"]\n",
    "    \n",
    "#     # load data\n",
    "#     data, train_loader = load_dataset()\n",
    "#     model = Model(data.num_features, n_hidden, learning_rate)\n",
    "\n",
    "#     losses, ndcgs = [], []\n",
    "#     for epoch in range(epochs):\n",
    "#         eval_count = 0\n",
    "#         for x_batch, y_batch in train_loader:\n",
    "#             model = train_batch(x_batch, y_batch, model)\n",
    "#             eval_count +=1\n",
    "#             if eval_count % 2000 == 0:\n",
    "#                 loss, scores = eval_model(model, data.validation)\n",
    "#                 losses.append(loss)\n",
    "#                 ndcgs.append(scores[\"ndcg\"][0])\n",
    "#         print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
    "        \n",
    "#     return ndcgs, losses, model\n",
    "\n",
    "\n",
    "def get_distributions(model):\n",
    "    data = dataset.get_dataset().get_data_folds()[0]\n",
    "    data.read_data()\n",
    "    model.ranknet.eval()\n",
    "\n",
    "    val_x = torch.from_numpy(data.validation.feature_matrix).float().to(device)\n",
    "    test_x = torch.from_numpy(data.test.feature_matrix).float().to(device)\n",
    "           \n",
    "    val = model.ranknet(val_x).detach().cpu().numpy().squeeze()\n",
    "    test = model.ranknet(test_x).detach().cpu().numpy().squeeze()\n",
    "    actual = np.concatenate((data.train.label_vector, data.validation.label_vector, data.test.label_vector))\n",
    "    \n",
    "    distributions = {\n",
    "    \"val_mean\": np.mean(val),\n",
    "    \"val_std\": np.std(val),\n",
    "    \"test_mean\": np.mean(test),\n",
    "    \"test_std\": np.std(test),\n",
    "    \"actual_mean\": np.mean(actual), \n",
    "    \"actual_std\": np.std(actual),\n",
    "    }\n",
    "    \n",
    "    return distributions\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #determine best hyper parameters\n",
    "    best_params = hyperparam_search()\n",
    "    #train best model\n",
    "    ndcgs, losses, model = train_best(best_params)\n",
    "    #plot ndcg and loss    \n",
    "    plot_ndcg_loss(losses, ndcgs)\n",
    "    #get distributions of scores\n",
    "    distributions = get_distributions(model)\n",
    "    #performance on test set\n",
    "    data = dataset.get_dataset().get_data_folds()[0]\n",
    "    data.read_data()\n",
    "    loss, scores = eval_model(model, data.test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "PYr5Lqm6ausr",
    "outputId": "f08fb301-0be3-42b1-cea7-1a4f270bf7c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "class RankNetSU(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.output = torch.nn.Linear(n_hidden, 1)      \n",
    "        \n",
    "    def forward(self, x1):\n",
    "        x = torch.nn.functional.relu(self.hidden(x1))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, n_feature, n_hidden, learning_rate):\n",
    "        self.ranknet = RankNetSU(n_feature, n_hidden)\n",
    "        #.to(device)\n",
    "        self.optimizer = torch.optim.SGD(self.ranknet.parameters(), lr=learning_rate)\n",
    "\n",
    "def eval_model(model, data_fold):\n",
    "    #with torch.no_grad():\n",
    "        x = torch.from_numpy(data_fold.feature_matrix).float()\n",
    "        y = data_fold.label_vector\n",
    "        model.ranknet.eval()\n",
    "               \n",
    "        output = model.ranknet(x)\n",
    "        output = output.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        #loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-y)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
    "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    train_x = torch.from_numpy(data.train.feature_matrix).float()\n",
    "    train_y = torch.from_numpy(data.train.label_vector).float()\n",
    "\n",
    "    train_set = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "\n",
    "    return train_loader\n",
    "       \n",
    "\n",
    "def plot_ndcg_loss(losses, ndcgs):\n",
    "    x = np.arange(len(losses))\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(x, losses, label='Loss')\n",
    "    ax.plot(x, ndcgs, label='NDCG')\n",
    "    ax.set_xlabel(\"Batch % 2000\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Pointwise LTR\")\n",
    "    legend = ax.legend(loc='upper center')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.savefig('Pointwise_LTR_plot.png')\n",
    "\n",
    "    \n",
    "def train_batch(documentfeatures, labels, model):\n",
    "#     model.ranknet.train()\n",
    "    for epoch in range(1):\n",
    "#         for qid in range(0, train_data.num_queries()):\n",
    "#             if train_data.query_size(qid) < 2:\n",
    "#                 continue\n",
    " \n",
    "            model.optimizer.zero_grad()\n",
    "\n",
    "            output = model.ranknet(documentfeatures)\n",
    "            \n",
    "            loss = pairwiseloss(output, labels)\n",
    "            break\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            model.optimizer.step()\n",
    "            \n",
    "    return model\n",
    "    \n",
    "def pairwiseloss(predictedvals, values):\n",
    "    \n",
    "    predictedvals = predictedvals.squeeze()\n",
    "\n",
    "    if predictedvals.shape[0] == 0:\n",
    "        return torch.tensor([0.0], requires_grad= True)\n",
    "    tups = list(itertools.combinations(range(predictedvals.shape[0]), 2))\n",
    "    \n",
    "\n",
    "    val1, val2 = [x[0] for x in tups], [x[1] for x in tups]\n",
    "    #todevice\n",
    "    pred1 = predictedvals[val1]\n",
    "    pred2 = predictedvals[val2]\n",
    "    \n",
    "    true1 = values[val1]\n",
    "    true2 = values[val2]\n",
    "    #.todevice\n",
    "    l1 = (true1 > true2).type(torch.ByteTensor)\n",
    "    l2 = (true1 < true2).type(torch.ByteTensor)\n",
    "    \n",
    "    S =  l1 - l2\n",
    "    S = torch.tensor(S)\n",
    "    sigmoid = torch.sigmoid(pred1.float() - pred2.float())\n",
    "#   C_T = (0.5 * (1 - S) * sigmoid + torch.log(1 + torch.exp(-sigmoid)))\n",
    "    lambda_ij = (torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+sigmoid))).sigmoid()\n",
    "    print(lambda_ij.shape)\n",
    "#     lambda_ij = torch.sigmoid(torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S)torch.log(torch.FloatTensor([1])+torch.exp(-((s_i-s_j).sigmoid().view(s_i.size(0)))))\n",
    "\n",
    "    return lambda_ij.sum()\n",
    "\n",
    "def hyperparam_search():\n",
    "    # hyper-parameters\n",
    "    epochs = 10\n",
    "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
    "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
    "#     learning_rates = [0.001]\n",
    "#     n_hiddens = [100]\n",
    "    print(\"hi\")\n",
    "#     train_loader = load_dataset()\n",
    "    print(\"2\")\n",
    "    best_ndcg = 0\n",
    "    for learning_rate in learning_rates:\n",
    "        for n_hidden in n_hiddens:\n",
    "        \n",
    "            print(\"\\nTesting learning_rate = {} and n_hidden = {}\".format(learning_rate, n_hidden))\n",
    "            model = Model(data.num_features, n_hidden, learning_rate)\n",
    "            \n",
    "            last_ndcg = 0\n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                model.ranknet.train()\n",
    "                for qid in range(0, data.train.num_queries()):#\n",
    "                    if data.train.query_size(qid) < 2:#\n",
    "                        continue#\n",
    "                    s_i, e_i = data.train.query_range(qid)\n",
    "            \n",
    "                    documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
    "                    labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
    "#                     if documentfeatures.shape[0] == 0:\n",
    "#                         return torch.tensor([0.0], requires_grad= True)\n",
    "\n",
    "                    model = train_batch(documentfeatures, labels, model)  \n",
    "\n",
    "                                       \n",
    "                scores = eval_model(model, data.validation)\n",
    "              \n",
    "                ndcg = scores[\"ndcg\"][0]\n",
    "                print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
    "                            \n",
    "                if ndcg < last_ndcg:\n",
    "                    break\n",
    "                last_ndcg = ndcg\n",
    "                if ndcg > best_ndcg:\n",
    "                    best_ndcg = ndcg\n",
    "                    best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch}            \n",
    "                    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "hyperparam_search()\n",
    "\n",
    "\n",
    "\n",
    "#### Loss functie 1tje kiezen voor validation en train.\n",
    "####  \n",
    "\n",
    "# {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 8}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wv0Oqd4akqP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNNyCVQ5akqa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4jxUhn8akqn"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "sped_ndcg, sped_arr = [], []\n",
    "class SpedUpRanknet():\n",
    "  def __init__(self, num_features, hidden_units=10, device=\"cpu\", lr = 0.05, layers = 1,  early_stopping=15): #, encoder, decoder, classes, n_protos, bottleneck_size=40, device='cpu', model_type='simple'\n",
    "    '''\n",
    "    arguments:\n",
    "      num_features = size of input vector (features size)\n",
    "    '''\n",
    "    self.normal_ndcg = = [0 for _ in range(early_stopping)]\n",
    "    self.num_features = num_features\n",
    "    self.lr = lr\n",
    "    assert layers in [1,2,3,4], \"layers need to be 1,2,3 or 4\"\n",
    "    if layers == 2:\n",
    "      self.model = nn.Sequential(\n",
    "              nn.Linear(num_features, hidden_units),\n",
    "              nn.Dropout(0.5),\n",
    "              nn.BatchNorm1d(hidden_units),\n",
    "              nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(hidden_units, 1)\n",
    "          )\n",
    "    elif layers == 1:\n",
    "      self.model = nn.Sequential(\n",
    "              nn.Linear(num_features, 1)\n",
    "          )\n",
    "    elif layers == 3:\n",
    "      self.model = nn.Sequential(\n",
    "              nn.Linear(num_features, int(hidden_units/2)),\n",
    "              nn.Dropout(0.5),\n",
    "              nn.BatchNorm1d(int(hidden_units/2)),\n",
    "              nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(int(hidden_units/2), hidden_units),\n",
    "              nn.Dropout(0.5),\n",
    "              nn.BatchNorm1d(hidden_units),\n",
    "              nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(hidden_units, 1)\n",
    "          )\n",
    "    elif layers == 4:\n",
    "      self.model = nn.Sequential(\n",
    "              nn.Linear(num_features, int(hidden_units/2)),\n",
    "              nn.Dropout(0.5),\n",
    "              nn.BatchNorm1d(int(hidden_units/2)),\n",
    "              nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(int(hidden_units/2), hidden_units),\n",
    "              nn.Dropout(0.5),\n",
    "              nn.BatchNorm1d(hidden_units),\n",
    "              nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(hidden_units, int(hidden_units/2)),\n",
    "              nn.Dropout(0.5),\n",
    "              nn.BatchNorm1d(int(hidden_units/2)),\n",
    "              nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(int(hidden_units/2), 1)\n",
    "          )\n",
    "\n",
    "\n",
    "    self.output_sig = nn.Sigmoid()\n",
    "    self.device=device\n",
    "    self.model.to(device)\n",
    "    for mod in self.model:\n",
    "      if isinstance(mod, nn.Linear):\n",
    "        nn.init.normal_(mod.weight, mean=2, std=2)\n",
    "        mod.bias.data.fill_(0)\n",
    "  \n",
    "  def forward(self, input_1):\n",
    "    '''\n",
    "    Does one forward pass using two inputs\n",
    "    '''\n",
    "    si = self.model(input_1.float())\n",
    "    # sj = self.model(input_2.float())#self.output_sig(x)\n",
    "    \n",
    "    return si\n",
    "\n",
    "  # def get_feature_vectors_target_train(self, s_i, e_i):\n",
    "  #   docs = data.train.feature_matrix[s_i:e_i]\n",
    "  #   document_list = list(range(len(docs)))\n",
    "\n",
    "  #   # create dict from idx of list location to the feature vector\n",
    "  #   idx_2_fm = {}\n",
    "  #   for e, d in enumerate(docs):\n",
    "  #     idx_2_fm[e] = d\n",
    "\n",
    "  #   # create all the possible combinations\n",
    "  #   doc_combs_in_qid = list(itertools.combinations(document_list,2))\n",
    "  #   input_1, input_2, target = [], [], []\n",
    "  \n",
    "  #   # iterate over all possible combinations\n",
    "  #   for i,j in doc_combs_in_qid:\n",
    "  #     input_1.append(docs[i])\n",
    "  #     input_2.append(docs[j])\n",
    "  #     if data.train.label_vector[i+s_i]>data.train.label_vector[j+s_i]:\n",
    "  #       # this is the S_{ij}\n",
    "  #       target.append(float(1))\n",
    "  #     elif data.train.label_vector[i+s_i]<data.train.label_vector[j+s_i]:\n",
    "  #       target.append(float(-1))\n",
    "  #     else:\n",
    "  #       target.append(float(0))\n",
    "  #   return torch.FloatTensor(input_1), torch.FloatTensor(input_2), torch.FloatTensor(target)\n",
    "\n",
    "  def fast_loss(self, forward_output, first_doc, last_doc):\n",
    "    # docs = data.train.feature_matrix[s_i:e_i]\n",
    "    document_list = list(range(last_doc-first_doc))\n",
    "\n",
    "    # idx_2_fm = {}\n",
    "    # for e, d in enumerate(docs):\n",
    "    #   idx_2_fm[e] = d\n",
    "\n",
    "    doc_combs_in_qid = list(itertools.combinations(document_list,2))\n",
    "    q_loss = torch.zeros(1) + (float(0)*forward_output[0])\n",
    "    for i,j in doc_combs_in_qid:\n",
    "      s_i = forward_output[i]\n",
    "      s_j = forward_output[j]\n",
    "      if data.train.label_vector[i+first_doc]>data.train.label_vector[j+first_doc]:\n",
    "        # this is the S_{ij}\n",
    "        S_ij = float(1)\n",
    "      elif data.train.label_vector[i+first_doc]<data.train.label_vector[j+first_doc]:\n",
    "        S_ij = float(-1)\n",
    "      else:\n",
    "        continue\n",
    "      loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S_ij)*((s_i-s_j).sigmoid()).view(s_i.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((s_i-s_j).sigmoid().view(s_i.size(0)))))\n",
    "\n",
    "      # loss = (torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S_ij)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+torch.exp((s_i-s_j).sigmoid().view(s_i.size(0)))))).sigmoid()\n",
    "      q_loss += loss\n",
    "    # print(q_loss)\n",
    "    return q_loss\n",
    "\n",
    "  def train(self, n_epochs):\n",
    "    lr=self.lr\n",
    "    if not os.path.exists('loss_folder'):\n",
    "        os.makedirs('loss_folder')\n",
    "\n",
    "    optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    losses = {'Ranking_class': [] }\n",
    "    accuracies = {'Test': [], 'Train': [] }\n",
    "    # data arguments: ['datafold', 'doc_feat', 'doc_str', 'doclist_ranges', 'feature_matrix', 'label_vector', 'name', 'num_docs', 'num_queries', 'query_feat', 'query_labels', 'query_range', 'query_size', 'query_sizes']\n",
    "    for epoch in range(n_epochs):\n",
    "        c_loss = []\n",
    "        train_acc = []\n",
    "        \n",
    "        num_queries = list(range(1, data.train.num_queries()))\n",
    "        # if epoch != 0:\n",
    "        #   random.shuffle(num_queries)\n",
    "        # num_queries = num_queries[0 : int(len(num_queries) * early_stop)]\n",
    "        for qid in num_queries:\n",
    "            s_i, e_i = data.train.query_range(qid)\n",
    "\n",
    "            # input_1, input_2, target = self.get_feature_vectors_target_train(s_i, e_i)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_1 = torch.FloatTensor(data.train.feature_matrix[s_i:e_i])\n",
    "            try:\n",
    "              si = self.forward(input_1)\n",
    "            except:\n",
    "              print(\"Query with one document, skipping this query\", qid)\n",
    "              continue\n",
    "\n",
    "            loss = self.fast_loss(si, s_i, e_i)\n",
    "\n",
    "            # loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-target)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
    "            # loss = (torch.FloatTensor([0.5])*(torch.FloatTensor([1])-target)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+torch.exp((si-sj).sigmoid().view(si.size(0)))))).sigmoid()\n",
    "\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if not qid%100:\n",
    "              print(f\"Data: {qid}, with a loss of {loss.sum()}\")\n",
    "              all_scores = self.model(torch.FloatTensor(data.validation.feature_matrix)).view(-1).detach().numpy()\n",
    "              metrics = self.evaluate(data.validation, all_scores)\n",
    "              ndcg = metrics['ndcg']\n",
    "              arr = metrics['relevant rank']\n",
    "              sped_ndcg.append(ndcg)\n",
    "              sped_arr.append(arr)\n",
    "              print(f\"the ndcg is {ndcg} the arr is {arr}\")\n",
    "              self.normal_ndcg.append(ndcg[0])\n",
    "              self.normal_ndcg.pop(0)\n",
    "              x = list(range(0, len(self.normal_ndcg)))\n",
    "              slope, *_ = stats.linregress(x, self.normal_ndcg)\n",
    "              if slope < 0:\n",
    "                print(f\"negative slope, early stopping activated slope of: {slope}\")\n",
    "                break\n",
    "\n",
    "    all_scores = self.model(torch.FloatTensor(data.train.feature_matrix)).view(-1).detach().numpy()\n",
    "    metrics = self.evaluate(data.train, all_scores)\n",
    "    ndcg = metrics['ndcg']\n",
    "    print(f\"Train ndcg is {ndcg}\")\n",
    "\n",
    "    # create_log(losses, accuracies, log_file)\n",
    "    # torch.save(model.state_dict(), os.path.join(model_folder, 'model_final.pt'))\n",
    "\n",
    "  def predict(self, data_split):\n",
    "      # self.eval()\n",
    "      allscores = []\n",
    "      x_val_tensor = torch.from_numpy(data_split.feature_matrix).float()\n",
    "      y_val_tensor = torch.from_numpy(data_split.label_vector)\n",
    "      \n",
    "      count = 0 \n",
    "      for val, label in zip(x_val_tensor, y_val_tensor):\n",
    "          y_pred = self.model(val)\n",
    "          allscores.append(y_pred.numpy())\n",
    "          \n",
    "      return np.array(allscores)\n",
    "\n",
    "  def evaluate(self, data_split, all_scores):\n",
    "    '''\n",
    "    function to evaluate the model\n",
    "    '''\n",
    "    metrics = evl.evaluate(data_split, all_scores, False)\n",
    "    return metrics\n",
    "\n",
    "sped_model = SpedUpRanknet(num_features=data.num_features, hidden_units=10, lr = 0.05, layers = 1, device='cpu')\n",
    "# print(\"hier\")\n",
    "sped_model.train(n_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PairwiseLTR-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
