{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PairwiseLTR_working.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GhNC8yFuD2eo",
        "outputId": "1fd76c62-3318-43ca-962d-88e3aca84ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "% cd drive/My Drive/hw3/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/hw3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gAbI82HvEJgL",
        "colab": {}
      },
      "source": [
        "import dataset\n",
        "import ranking as rnk\n",
        "import evaluate as evl\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import itertools\n",
        "\n",
        "import math\n",
        "data = dataset.get_dataset().get_data_folds()[0]\n",
        "data.read_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ILMI8Kmhakp5",
        "colab": {}
      },
      "source": [
        "\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#torch.manual_seed(0)\n",
        "\n",
        "class RankNet(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.output = torch.nn.Linear(n_hidden, 1)      \n",
        "        \n",
        "    def forward(self, x1):\n",
        "        x = torch.nn.functional.relu(self.hidden(x1))\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, n_feature, n_hidden, learning_rate, sigma):\n",
        "        self.ranknet = RankNet(n_feature, n_hidden)\n",
        "        #.to(device)\n",
        "        self.optimizer = torch.optim.SGD(self.ranknet.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_model(model, data_fold):\n",
        "    #with torch.no_grad():\n",
        "        x = torch.from_numpy(data_fold.feature_matrix).float()\n",
        "        y = data_fold.label_vector\n",
        "        model.ranknet.eval()\n",
        "               \n",
        "        output = model.ranknet(x)\n",
        "        output = output.detach().cpu().numpy().squeeze()\n",
        "        \n",
        "        #loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-y)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
        "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    # data = dataset.get_dataset().get_data_folds()[0]\n",
        "    # data.read_data()\n",
        "\n",
        "    train_x = torch.from_numpy(data.train.feature_matrix).float()\n",
        "    train_y = torch.from_numpy(data.train.label_vector).float()\n",
        "\n",
        "    # documents = data.train.feature_matrix\n",
        "    # doc_list = list(range(len(documents)))\n",
        "    \n",
        "    # # Carthesian product\n",
        "    # Carth = list(itertools.combinations(doc_list,2))\n",
        "    # x1, x2, target = [], [], []\n",
        "  \n",
        "    # # iterate over all possible combinations\n",
        "    # for i,j in Carth:\n",
        "    #     x1.append(docs[i])\n",
        "    #     x2.append(docs[j])\n",
        "    #     if data.train.label_vector[i]>data.train.label_vector[j]:\n",
        "    #         # this is the S_{ij}\n",
        "    #         target.append(float(1))\n",
        "    #     elif data.train.label_vector[i]<data.train.label_vector[j]:\n",
        "    #         target.append(float(-1))\n",
        "    #     else:\n",
        "    #         target.append(float(0))\n",
        "\n",
        "\n",
        "    train_set = TensorDataset(train_x, train_y)\n",
        "    train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
        "#     return torch.FloatTensor(x1), torch.FloatTensor(x2), torch.FloatTensor(target)\n",
        "    #return data\n",
        "    return train_loader\n",
        "       \n",
        "\n",
        "def plot_ndcg_loss(ndcgs):\n",
        "    x = np.arange(len(ndcgs))\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    ax.plot(x, ndcgs, label='NDCG')\n",
        "    ax.set_xlabel(\"Batch % 2000\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_title(\"Pointwise LTR\")\n",
        "    legend = ax.legend(loc='upper center')\n",
        "    \n",
        "    plt.show()\n",
        "    plt.savefig('Pairwise_LTR_plot.png')\n",
        "\n",
        "    \n",
        "def train_batch(documentfeatures, labels, model, sig):\n",
        "#     model.ranknet.train()\n",
        "    for epoch in range(1):\n",
        "#         for qid in range(0, train_data.num_queries()):\n",
        "#             if train_data.query_size(qid) < 2:\n",
        "#                 continue\n",
        " \n",
        "            model.optimizer.zero_grad()\n",
        "\n",
        "            output = model.ranknet(documentfeatures)\n",
        "            \n",
        "            loss = pairwiseloss(output, labels, sig)\n",
        "        \n",
        "            loss.backward()\n",
        "            \n",
        "            model.optimizer.step()\n",
        "            \n",
        "    return model\n",
        "    \n",
        "def pairwiseloss(predictedvals, values, sig):\n",
        "    \n",
        "    predictedvals = predictedvals.squeeze()\n",
        "\n",
        "    # Hier is waar er is gesleuteld aan de data om de nan's te voorkomen\n",
        "    if predictedvals.shape[0] == 0:\n",
        "        return torch.tensor([0.0], requires_grad= True)\n",
        "    # pairs = int(math.factorial(n_docs) / (math.factorial(n_docs - 2) * 2))\n",
        "    tups = list(itertools.combinations(range(predictedvals.shape[0]), 2))\n",
        "    \n",
        "\n",
        "    val1, val2 = [x[0] for x in tups], [x[1] for x in tups]\n",
        "    #todevice\n",
        "    pred1 = predictedvals[val1]\n",
        "    pred2 = predictedvals[val2]\n",
        "    \n",
        "    true1 = values[val1]\n",
        "    true2 = values[val2]\n",
        "    #.todevice\n",
        "    s1 = (true1 > true2).type(torch.FloatTensor)\n",
        "    s2 = (true1 < true2).type(torch.FloatTensor)\n",
        "    \n",
        "    S =  s1 - s2\n",
        "    S = torch.tensor(S)\n",
        "    sigma = sig\n",
        "    C_T = (0.5 * (1 - S) * sigma * (s1-s2) + torch.log(1 + torch.exp(-sigma*(s1-s2))))\n",
        "    C_T = torch.tensor(C_T, requires_grad = True)\n",
        "    \n",
        "\n",
        "    return C_T.mean()\n",
        "\n",
        "def hyperparam_search():\n",
        "    # hyper-parameters\n",
        "    epochs = 10\n",
        "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
        "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
        "#     learning_rates = [ 10**-1]\n",
        "#     n_hiddens = [150, 200, 250]\n",
        "    sig = [0.1, 1, 10, 100]\n",
        "    print(\"hi\")\n",
        "#     train_loader = load_dataset()\n",
        "    print(\"2\")\n",
        "    best_ndcg = 0\n",
        "    for learning_rate in learning_rates:\n",
        "        for n_hidden in n_hiddens:\n",
        "            for sigma in sig:\n",
        "\n",
        "                print(\"\\nTesting learning_rate = {}, n_hidden = {} and sigma = {}\".format(learning_rate, n_hidden, sigma))\n",
        "                model = Model(data.num_features, n_hidden, learning_rate, sigma)\n",
        "\n",
        "                last_ndcg = 0\n",
        "                for epoch in range(epochs):\n",
        "\n",
        "                    model.ranknet.train()\n",
        "                    for qid in range(0, data.train.num_queries()):#\n",
        "                        if data.train.query_size(qid) < 2:#\n",
        "                            continue#\n",
        "                        s_i, e_i = data.train.query_range(qid)\n",
        "\n",
        "                        documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
        "                        labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
        "    #                     if documentfeatures.shape[0] == 0:\n",
        "    #                         return torch.tensor([0.0], requires_grad= True)\n",
        "\n",
        "                        model = train_batch(documentfeatures, labels, model, sigma)  \n",
        "\n",
        "                                       \n",
        "                    scores = eval_model(model, data.validation)\n",
        "              \n",
        "                    ndcg = scores[\"ndcg\"][0]\n",
        "                    print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
        "                            \n",
        "                    if ndcg < last_ndcg:\n",
        "                        break\n",
        "                    last_ndcg = ndcg\n",
        "                    if ndcg > best_ndcg:\n",
        "                        best_ndcg = ndcg\n",
        "                        best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch, \"sigma\": sigma}            \n",
        "                        print(\"Best parameters:\", best_params)\n",
        "    \n",
        "    return best_params\n",
        "\n",
        "# hyperparam_search()\n",
        "\n",
        "\n",
        "\n",
        "#### Loss functie 1tje kiezen voor validation en train.\n",
        "####  \n",
        "\n",
        "# {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 8}\n",
        "# {'learning_rate': 0.1, 'n_hidden': 250, 'epoch': 6}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "veVi9--TlXx6",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_best(best_params):\n",
        "    epochs = best_params[\"epoch\"]\n",
        "    n_hidden = best_params[\"n_hidden\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    sigma = best_params[\"sigma\"]\n",
        "    \n",
        "    # load data\n",
        "#     data, train_loader = load_dataset()\n",
        "    model = Model(data.num_features, n_hidden, learning_rate, sigma)\n",
        "\n",
        "    losses, ndcgs = [], []\n",
        "    for epoch in range(epochs):\n",
        "        eval_count = 0\n",
        "        for qid in range(0, data.train.num_queries()):#\n",
        "            if data.train.query_size(qid) < 2:#\n",
        "                continue#\n",
        "            s_i, e_i = data.train.query_range(qid)\n",
        "            \n",
        "            documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
        "            labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
        "            model = train_batch(documentfeatures, labels, model, sigma) \n",
        "            eval_count +=1\n",
        "            if eval_count % 2000 == 0:\n",
        "                scores = eval_model(model, data.validation)\n",
        "                ndcgs.append(scores[\"ndcg\"][0])\n",
        "        print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
        "        \n",
        "    return ndcgs, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uy_km10Varfn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6f13329-ddfa-411f-c32a-e49fd3259a9a"
      },
      "source": [
        "\n",
        "# def train_best(best_params):\n",
        "#     epochs = best_params[\"epoch\"]\n",
        "#     n_hidden = best_params[\"n_hidden\"]\n",
        "#     learning_rate = best_params[\"learning_rate\"]\n",
        "    \n",
        "#     # load data\n",
        "#     data, train_loader = load_dataset()\n",
        "#     model = Model(data.num_features, n_hidden, learning_rate)\n",
        "\n",
        "#     losses, ndcgs = [], []\n",
        "#     for epoch in range(epochs):\n",
        "#         eval_count = 0\n",
        "#         for x_batch, y_batch in train_loader:\n",
        "#             model = train_batch(x_batch, y_batch, model)\n",
        "#             eval_count +=1\n",
        "#             if eval_count % 2000 == 0:\n",
        "#                 loss, scores = eval_model(model, data.validation)\n",
        "#                 losses.append(loss)\n",
        "#                 ndcgs.append(scores[\"ndcg\"][0])\n",
        "#         print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
        "        \n",
        "#     return ndcgs, losses, model\n",
        "\n",
        "\n",
        "def get_distributions(model):\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    model.ranknet.eval()\n",
        "\n",
        "    val_x = torch.from_numpy(data.validation.feature_matrix).float()\n",
        "    test_x = torch.from_numpy(data.test.feature_matrix).float()\n",
        "           \n",
        "    val = model.ranknet(val_x).detach().cpu().numpy().squeeze()\n",
        "    test = model.ranknet(test_x).detach().cpu().numpy().squeeze()\n",
        "    actual = np.concatenate((data.train.label_vector, data.validation.label_vector, data.test.label_vector))\n",
        "    \n",
        "    distributions = {\n",
        "    \"val_mean\": np.mean(val),\n",
        "    \"val_std\": np.std(val),\n",
        "    \"test_mean\": np.mean(test),\n",
        "    \"test_std\": np.std(test),\n",
        "    \"actual_mean\": np.mean(actual), \n",
        "    \"actual_std\": np.std(actual),\n",
        "    }\n",
        "    \n",
        "    return distributions\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #determine best hyper parameters\n",
        "    best_params = hyperparam_search()\n",
        "    #train best model\n",
        "    ndcgs, model = train_best(best_params)\n",
        "    #plot ndcg and loss    \n",
        "    plot_ndcg_loss(ndcgs)\n",
        "    #get distributions of scores\n",
        "    distributions = get_distributions(model)\n",
        "    #performance on test set\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    scores = eval_model(model, data.test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "2\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 100 and sigma = 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6800047414120663\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 0, 'sigma': 0.1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.680023760765308\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 1, 'sigma': 0.1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6800222422737341\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 100 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7345485029103731\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 0, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7345841144020738\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 1, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7345408034694144\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 100 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7149874904659647\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7150142239753081\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7149123069773602\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 100 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6842174130824217\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6842439605283318\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6842051011033998\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 150 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6917496325930141\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6917635140407947\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.691791436446827\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.6917235446089054\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 150 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7513655754356358\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 0, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7513898920678592\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 1, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7514469357198594\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 2, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7512794357100614\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 150 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7253230360243773\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7254063225907309\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7252309764069771\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 150 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7081559548229093\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7081551004404943\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 200 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7401766677271822\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7401711904432461\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 200 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.737374046973005\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7373070070868802\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 200 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6834586500830319\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6834628013085889\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6835044880140589\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.6835110111232902\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.6834851659131752\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 200 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7006335459040307\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7006225644088728\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 250 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7383094234321872\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7383196200735196\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7383201337384899\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7383452615390894\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.7383786152331289\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 5, ndcg: 0.7383183133524523\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 250 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7106881580881298\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7106403621648321\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 250 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7114848993523508\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7115320126289285\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.711543516895725\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7115910577794708\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.7115696485611718\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 250 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7094598928017811\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7095199097475549\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7095069305540208\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 300 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7077711985446474\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.707739299768178\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 300 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7089454218927604\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7089503708463192\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.708971745674151\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7089726669311778\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.708950991631849\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 300 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6961529672214062\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6961870361065986\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6962019198140298\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.696175656606919\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 300 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.72901447619429\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7289720333078844\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 350 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7125957042116714\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7125922106022671\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 350 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7124355360060683\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7124472970829326\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7124819143449804\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.712377317591082\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 350 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7264975794058302\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7265247498942481\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7264649365512895\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 350 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.705202312501344\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7053203333428525\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7052877510958182\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 400 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6962259510350184\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6962624634834164\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6961811479809482\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 400 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.773051901566433\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 400, 'epoch': 0, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7731728785865389\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 400, 'epoch': 1, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7730551084820567\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 400 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6956431428527193\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6955756952324302\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 400 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6990992349520686\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6991186096931745\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6990511168258523\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 100 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7426662141550529\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7426705895641711\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7427055762427304\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7427038851753998\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 100 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.765149522342238\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7651321117550495\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 100 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7000363485794614\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7000127171106687\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 100 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7397757361182594\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7398567317091426\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7397916562643205\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 150 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.686274056258157\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6862829748033373\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6861997728317909\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 150 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7351229733033113\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7350474950791175\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 150 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7154744263700037\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7154926452481238\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7153894175622135\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 150 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7390058095130855\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7389185166932437\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 200 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7165229334282822\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7165864176555937\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7165687207853423\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 200 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7600048124633985\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7598751363411824\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 200 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7252937400280371\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7252410158830594\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 200 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7296684769703978\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7297767637666452\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7298003647627512\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7297904471931445\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 250 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7037036223700831\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7036945598677998\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 250 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.747003243806829\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7470335000769032\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7468861615266271\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 250 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7363813272359453\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7365355463006351\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.736558109639032\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7364413157763019\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 250 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6879737669026645\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6878658928093289\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 300 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7191082090640225\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7190165219795447\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 300 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6786265430994394\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6785742804063662\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 300 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7181402550161169\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7181598330402946\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7181414950987112\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 300 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7131209888836326\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7130891178623323\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 350 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7191505612102469\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7191011803777064\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 350 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7379473618241894\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7379156955430423\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 350 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6747487452080125\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.674790892121539\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6748233028992192\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.674788834261937\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 350 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6695946631199765\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6695825176178387\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 400 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7110664286933424\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.711066685084345\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7110764077777433\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7110908877089532\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.7110547300461673\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 400 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6953736902272831\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6953602451332277\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 400 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7097752887711054\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.709792853593702\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7097443218795836\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 400 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7201800649335397\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7200927673567153\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 100 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6851122563238428\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6851035727336522\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 100 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7301279344607958\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7301361593448031\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7300316573660476\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 100 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7110511679339673\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.711148395451063\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7111518458029261\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7110758358513168\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 100 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6952909450639514\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6952523602211979\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 150 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7223160425796286\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7224454351804607\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7222678822981092\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 150 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6963459523892968\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.696393140981516\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6963288413406781\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 150 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7321548806825243\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7320998067192551\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 150 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7242548039386772\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7241704929345257\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 200 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7441959975782422\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7441634857353445\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 200 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6860841259420564\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6860752559337726\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 200 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7164392851998063\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7164267915804676\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 200 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6962766044485078\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6963310528732037\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6963034429777403\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 250 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7052570796537994\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7054077970205322\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7053575505394278\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 250 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7373932944690078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PYr5Lqm6ausr",
        "outputId": "e056e282-5bb6-421e-ff81-3f63377c63e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#torch.manual_seed(0)\n",
        "\n",
        "class RankNetSU(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden):\n",
        "        super(RankNetSU, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.output = torch.nn.Linear(n_hidden, 1)      \n",
        "        \n",
        "    def forward(self, x1):\n",
        "        x = torch.nn.functional.relu(self.hidden(x1))\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, n_feature, n_hidden, learning_rate, sigma):\n",
        "        self.ranknet = RankNetSU(n_feature, n_hidden)\n",
        "        #.to(device)\n",
        "        self.optimizer = torch.optim.SGD(self.ranknet.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_model(model, data_fold):\n",
        "    #with torch.no_grad():\n",
        "        x = torch.from_numpy(data_fold.feature_matrix).float()\n",
        "        y = data_fold.label_vector\n",
        "        model.ranknet.eval()\n",
        "               \n",
        "        output = model.ranknet(x)\n",
        "        output = output.detach().cpu().numpy().squeeze()\n",
        "        \n",
        "        #loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-y)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
        "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "\n",
        "    train_x = torch.from_numpy(data.train.feature_matrix).float()\n",
        "    train_y = torch.from_numpy(data.train.label_vector).float()\n",
        "\n",
        "    train_set = TensorDataset(train_x, train_y)\n",
        "    train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "    return train_loader\n",
        "       \n",
        "\n",
        "def plot_ndcg_loss(losses, ndcgs):\n",
        "    x = np.arange(len(losses))\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    ax.plot(x, losses, label='Loss')\n",
        "    ax.plot(x, ndcgs, label='NDCG')\n",
        "    ax.set_xlabel(\"Batch % 2000\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_title(\"Pointwise LTR\")\n",
        "    legend = ax.legend(loc='upper center')\n",
        "    \n",
        "    plt.show()\n",
        "    plt.savefig('Pairwise_LTR_plot.png')\n",
        "\n",
        "    \n",
        "def train_batch(documentfeatures, labels, model, sig):\n",
        "#     model.ranknet.train()\n",
        "    for epoch in range(1):\n",
        "#         for qid in range(0, train_data.num_queries()):\n",
        "#             if train_data.query_size(qid) < 2:\n",
        "#                 continue\n",
        "            model.optimizer.zero_grad()\n",
        "\n",
        "            output = model.ranknet(documentfeatures)\n",
        "            \n",
        "            loss = pairwiseloss(output, labels, sig)\n",
        "            \n",
        "            # AttributeError: 'Tensor' object has no attribute 'forward'\n",
        "            \n",
        "            loss.sum().backward()\n",
        "            \n",
        "            model.optimizer.step()\n",
        "\n",
        "    return model\n",
        "    \n",
        "def pairwiseloss(predictedvals, values, sig):\n",
        "    \n",
        "    predictedvals = predictedvals.squeeze()\n",
        "\n",
        "    if predictedvals.shape[0] == 0:\n",
        "        return torch.tensor([0.0], requires_grad= True)\n",
        "    tups = list(itertools.combinations(range(predictedvals.shape[0]), 2))\n",
        "#     print(tups)\n",
        "    val1, val2 = [x[0] for x in tups], [x[1] for x in tups]\n",
        "    #todevice\n",
        "    pred1 = predictedvals[val1]\n",
        "    pred2 = predictedvals[val2]\n",
        "    \n",
        "    true1 = values[val1]\n",
        "    true2 = values[val2]\n",
        "    #.todevice\n",
        "    s1 = (true1 > true2).type(torch.FloatTensor)\n",
        "    s2 = (true1 < true2).type(torch.FloatTensor)\n",
        "    \n",
        "    S =  s1 - s2\n",
        "    S = torch.tensor(S)\n",
        "#     print(S)\n",
        "#     print(s1)\n",
        "#     print(s2)\n",
        "    sigma = sig\n",
        "#     print(val1)\n",
        "    \n",
        "    lambda_ij = sig*(torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+torch.exp((s1-s2)*sigma))))\n",
        "#     print(lambda_ij.shape)\n",
        "#     lambda_ij = torch.sigmoid(torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S)torch.log(torch.FloatTensor([1])+torch.exp(-((s_i-s_j).sigmoid().view(s_i.size(0)))))\n",
        "#     print(predictedvals.shape)\n",
        "    lambda_i = np.zeros(len(set(val1))+1)\n",
        "    n=0\n",
        "    for i in val1:\n",
        "        lambda_i[i]+= lambda_ij[n]\n",
        "        n += 1\n",
        "#     print(val2)\n",
        "    m = 0\n",
        "    for j in val2:\n",
        "        lambda_i[j]-= lambda_ij[m] \n",
        "        m += 1\n",
        "    lambda_i = torch.tensor(lambda_i, requires_grad=True)\n",
        "#     print(lambda_i)\n",
        "#     print(lambda_i.shape)\n",
        "    return predictedvals * lambda_i.detach()\n",
        "\n",
        "def hyperparam_search():\n",
        "    # hyper-parameters\n",
        "    epochs = 2\n",
        "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
        "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
        "    sigma = [0.1, 1, 10, 100]\n",
        "    # learning_rates = [10**-2]\n",
        "    # n_hiddens = [100]\n",
        "    # sigma = [1]\n",
        "    print(\"hi\")\n",
        "#     train_loader = load_dataset()\n",
        "    print(\"2\")\n",
        "    best_ndcg = 0\n",
        "    for learning_rate in learning_rates:\n",
        "        for n_hidden in n_hiddens:\n",
        "            for sig in sigma:\n",
        "        \n",
        "                print(\"\\nTesting learning_rate = {} and n_hidden = {}\".format(learning_rate, n_hidden))\n",
        "                model = Model(data.num_features, n_hidden, learning_rate, sig)\n",
        "\n",
        "                last_ndcg = 0\n",
        "                for epoch in range(epochs):\n",
        "\n",
        "                    model.ranknet.train()\n",
        "                    for qid in range(0, data.train.num_queries()):#\n",
        "                        if data.train.query_size(qid) < 2:#\n",
        "                            continue#\n",
        "                        s_i, e_i = data.train.query_range(qid)\n",
        "\n",
        "                        documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
        "                        labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
        "    #                     if documentfeatures.shape[0] == 0:\n",
        "    #                         return torch.tensor([0.0], requires_grad= True)\n",
        "\n",
        "                        model = train_batch(documentfeatures, labels, model, sig)  \n",
        "\n",
        "\n",
        "                    scores = eval_model(model, data.validation)\n",
        "\n",
        "                    ndcg = scores[\"ndcg\"][0]\n",
        "                    print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
        "\n",
        "                    if ndcg < last_ndcg:\n",
        "                        break\n",
        "                    last_ndcg = ndcg\n",
        "                    if ndcg > best_ndcg:\n",
        "                        best_ndcg = ndcg\n",
        "                        best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch, \"sigma\": sig}            \n",
        "                        print(\"Best parameters:\", best_params)\n",
        "    \n",
        "    return best_params\n",
        "\n",
        "hyperparam_search()\n",
        "\n",
        "\n",
        "#### Loss functie 1tje kiezen voor validation en train.\n",
        "####  \n",
        "\n",
        "# {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 8}\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "2\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7148170714395715\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 0, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7197894944525909\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 1, 'sigma': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 1, 'learning_rate': 0.1, 'n_hidden': 100, 'sigma': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6wv0Oqd4akqP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nNNyCVQ5akqa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c4jxUhn8akqn",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "sped_ndcg, sped_arr = [], []\n",
        "class SpedUpRanknet():\n",
        "  def __init__(self, num_features, hidden_units=10, device=\"cpu\", lr = 0.05, layers = 1,  early_stopping=15): #, encoder, decoder, classes, n_protos, bottleneck_size=40, device='cpu', model_type='simple'\n",
        "    '''\n",
        "    arguments:\n",
        "      num_features = size of input vector (features size)\n",
        "    '''\n",
        "    self.normal_ndcg = = [0 for _ in range(early_stopping)]\n",
        "    self.num_features = num_features\n",
        "    self.lr = lr\n",
        "    assert layers in [1,2,3,4], \"layers need to be 1,2,3 or 4\"\n",
        "    if layers == 2:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, 1)\n",
        "          )\n",
        "    elif layers == 1:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, 1)\n",
        "          )\n",
        "    elif layers == 3:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, 1)\n",
        "          )\n",
        "    elif layers == 4:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), 1)\n",
        "          )\n",
        "\n",
        "\n",
        "    self.output_sig = nn.Sigmoid()\n",
        "    self.device=device\n",
        "    self.model.to(device)\n",
        "    for mod in self.model:\n",
        "      if isinstance(mod, nn.Linear):\n",
        "        nn.init.normal_(mod.weight, mean=2, std=2)\n",
        "        mod.bias.data.fill_(0)\n",
        "  \n",
        "  def forward(self, input_1):\n",
        "    '''\n",
        "    Does one forward pass using two inputs\n",
        "    '''\n",
        "    si = self.model(input_1.float())\n",
        "    # sj = self.model(input_2.float())#self.output_sig(x)\n",
        "    \n",
        "    return si\n",
        "\n",
        "  # def get_feature_vectors_target_train(self, s_i, e_i):\n",
        "  #   docs = data.train.feature_matrix[s_i:e_i]\n",
        "  #   document_list = list(range(len(docs)))\n",
        "\n",
        "  #   # create dict from idx of list location to the feature vector\n",
        "  #   idx_2_fm = {}\n",
        "  #   for e, d in enumerate(docs):\n",
        "  #     idx_2_fm[e] = d\n",
        "\n",
        "  #   # create all the possible combinations\n",
        "  #   doc_combs_in_qid = list(itertools.combinations(document_list,2))\n",
        "  #   input_1, input_2, target = [], [], []\n",
        "  \n",
        "  #   # iterate over all possible combinations\n",
        "  #   for i,j in doc_combs_in_qid:\n",
        "  #     input_1.append(docs[i])\n",
        "  #     input_2.append(docs[j])\n",
        "  #     if data.train.label_vector[i+s_i]>data.train.label_vector[j+s_i]:\n",
        "  #       # this is the S_{ij}\n",
        "  #       target.append(float(1))\n",
        "  #     elif data.train.label_vector[i+s_i]<data.train.label_vector[j+s_i]:\n",
        "  #       target.append(float(-1))\n",
        "  #     else:\n",
        "  #       target.append(float(0))\n",
        "  #   return torch.FloatTensor(input_1), torch.FloatTensor(input_2), torch.FloatTensor(target)\n",
        "\n",
        "  def fast_loss(self, forward_output, first_doc, last_doc):\n",
        "    # docs = data.train.feature_matrix[s_i:e_i]\n",
        "    document_list = list(range(last_doc-first_doc))\n",
        "\n",
        "    # idx_2_fm = {}\n",
        "    # for e, d in enumerate(docs):\n",
        "    #   idx_2_fm[e] = d\n",
        "\n",
        "    doc_combs_in_qid = list(itertools.combinations(document_list,2))\n",
        "    q_loss = torch.zeros(1) + (float(0)*forward_output[0])\n",
        "    for i,j in doc_combs_in_qid:\n",
        "      s_i = forward_output[i]\n",
        "      s_j = forward_output[j]\n",
        "      if data.train.label_vector[i+first_doc]>data.train.label_vector[j+first_doc]:\n",
        "        # this is the S_{ij}\n",
        "        S_ij = float(1)\n",
        "      elif data.train.label_vector[i+first_doc]<data.train.label_vector[j+first_doc]:\n",
        "        S_ij = float(-1)\n",
        "      else:\n",
        "        continue\n",
        "      loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S_ij)*((s_i-s_j).sigmoid()).view(s_i.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((s_i-s_j).sigmoid().view(s_i.size(0)))))\n",
        "\n",
        "      # loss = (torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S_ij)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+torch.exp((s_i-s_j).sigmoid().view(s_i.size(0)))))).sigmoid()\n",
        "      q_loss += loss\n",
        "    # print(q_loss)\n",
        "    return q_loss\n",
        "\n",
        "  def train(self, n_epochs):\n",
        "    lr=self.lr\n",
        "    if not os.path.exists('loss_folder'):\n",
        "        os.makedirs('loss_folder')\n",
        "\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "    losses = {'Ranking_class': [] }\n",
        "    accuracies = {'Test': [], 'Train': [] }\n",
        "    # data arguments: ['datafold', 'doc_feat', 'doc_str', 'doclist_ranges', 'feature_matrix', 'label_vector', 'name', 'num_docs', 'num_queries', 'query_feat', 'query_labels', 'query_range', 'query_size', 'query_sizes']\n",
        "    for epoch in range(n_epochs):\n",
        "        c_loss = []\n",
        "        train_acc = []\n",
        "        \n",
        "        num_queries = list(range(1, data.train.num_queries()))\n",
        "        # if epoch != 0:\n",
        "        #   random.shuffle(num_queries)\n",
        "        # num_queries = num_queries[0 : int(len(num_queries) * early_stop)]\n",
        "        for qid in num_queries:\n",
        "            s_i, e_i = data.train.query_range(qid)\n",
        "\n",
        "            # input_1, input_2, target = self.get_feature_vectors_target_train(s_i, e_i)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            input_1 = torch.FloatTensor(data.train.feature_matrix[s_i:e_i])\n",
        "            try:\n",
        "              si = self.forward(input_1)\n",
        "            except:\n",
        "              print(\"Query with one document, skipping this query\", qid)\n",
        "              continue\n",
        "\n",
        "            loss = self.fast_loss(si, s_i, e_i)\n",
        "\n",
        "            # loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-target)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
        "            # loss = (torch.FloatTensor([0.5])*(torch.FloatTensor([1])-target)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+torch.exp((si-sj).sigmoid().view(si.size(0)))))).sigmoid()\n",
        "\n",
        "            loss.sum().backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if not qid%100:\n",
        "              print(f\"Data: {qid}, with a loss of {loss.sum()}\")\n",
        "              all_scores = self.model(torch.FloatTensor(data.validation.feature_matrix)).view(-1).detach().numpy()\n",
        "              metrics = self.evaluate(data.validation, all_scores)\n",
        "              ndcg = metrics['ndcg']\n",
        "              arr = metrics['relevant rank']\n",
        "              sped_ndcg.append(ndcg)\n",
        "              sped_arr.append(arr)\n",
        "              print(f\"the ndcg is {ndcg} the arr is {arr}\")\n",
        "              self.normal_ndcg.append(ndcg[0])\n",
        "              self.normal_ndcg.pop(0)\n",
        "              x = list(range(0, len(self.normal_ndcg)))\n",
        "              slope, *_ = stats.linregress(x, self.normal_ndcg)\n",
        "              if slope < 0:\n",
        "                print(f\"negative slope, early stopping activated slope of: {slope}\")\n",
        "                break\n",
        "\n",
        "    all_scores = self.model(torch.FloatTensor(data.train.feature_matrix)).view(-1).detach().numpy()\n",
        "    metrics = self.evaluate(data.train, all_scores)\n",
        "    ndcg = metrics['ndcg']\n",
        "    print(f\"Train ndcg is {ndcg}\")\n",
        "\n",
        "    # create_log(losses, accuracies, log_file)\n",
        "    # torch.save(model.state_dict(), os.path.join(model_folder, 'model_final.pt'))\n",
        "\n",
        "  def predict(self, data_split):\n",
        "      # self.eval()\n",
        "      allscores = []\n",
        "      x_val_tensor = torch.from_numpy(data_split.feature_matrix).float()\n",
        "      y_val_tensor = torch.from_numpy(data_split.label_vector)\n",
        "      \n",
        "      count = 0 \n",
        "      for val, label in zip(x_val_tensor, y_val_tensor):\n",
        "          y_pred = self.model(val)\n",
        "          allscores.append(y_pred.numpy())\n",
        "          \n",
        "      return np.array(allscores)\n",
        "\n",
        "  def evaluate(self, data_split, all_scores):\n",
        "    '''\n",
        "    function to evaluate the model\n",
        "    '''\n",
        "    metrics = evl.evaluate(data_split, all_scores, False)\n",
        "    return metrics\n",
        "\n",
        "sped_model = SpedUpRanknet(num_features=data.num_features, hidden_units=10, lr = 0.05, layers = 1, device='cpu')\n",
        "# print(\"hier\")\n",
        "sped_model.train(n_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}