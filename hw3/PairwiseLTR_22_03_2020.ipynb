{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PairwiseLTR_22-03-2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GhNC8yFuD2eo",
        "outputId": "fbd59bc8-2e40-42d9-d4c8-661541df16ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# % cd drive/My Drive/hw3/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gAbI82HvEJgL",
        "colab": {}
      },
      "source": [
        "import dataset\n",
        "import ranking as rnk\n",
        "import evaluate as evl\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import itertools\n",
        "\n",
        "import math\n",
        "data = dataset.get_dataset().get_data_folds()[0]\n",
        "data.read_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ILMI8Kmhakp5",
        "colab": {}
      },
      "source": [
        "\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#torch.manual_seed(0)\n",
        "\n",
        "class RankNet(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.output = torch.nn.Linear(n_hidden, 1)      \n",
        "        \n",
        "    def forward(self, x1):\n",
        "        x = torch.nn.functional.relu(self.hidden(x1))\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, n_feature, n_hidden, learning_rate, sigma):\n",
        "        self.ranknet = RankNet(n_feature, n_hidden)\n",
        "        #.to(device)\n",
        "        self.optimizer = torch.optim.SGD(self.ranknet.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_model(model, data_fold):\n",
        "    #with torch.no_grad():\n",
        "        x = torch.from_numpy(data_fold.feature_matrix).float()\n",
        "        y = data_fold.label_vector\n",
        "        model.ranknet.eval()\n",
        "               \n",
        "        output = model.ranknet(x)\n",
        "        output = output.detach().cpu().numpy().squeeze()\n",
        "        \n",
        "        #loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-y)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
        "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    # data = dataset.get_dataset().get_data_folds()[0]\n",
        "    # data.read_data()\n",
        "\n",
        "    train_x = torch.from_numpy(data.train.feature_matrix).float()\n",
        "    train_y = torch.from_numpy(data.train.label_vector).float()\n",
        "\n",
        "    # documents = data.train.feature_matrix\n",
        "    # doc_list = list(range(len(documents)))\n",
        "    \n",
        "    # # Carthesian product\n",
        "    # Carth = list(itertools.combinations(doc_list,2))\n",
        "    # x1, x2, target = [], [], []\n",
        "  \n",
        "    # # iterate over all possible combinations\n",
        "    # for i,j in Carth:\n",
        "    #     x1.append(docs[i])\n",
        "    #     x2.append(docs[j])\n",
        "    #     if data.train.label_vector[i]>data.train.label_vector[j]:\n",
        "    #         # this is the S_{ij}\n",
        "    #         target.append(float(1))\n",
        "    #     elif data.train.label_vector[i]<data.train.label_vector[j]:\n",
        "    #         target.append(float(-1))\n",
        "    #     else:\n",
        "    #         target.append(float(0))\n",
        "\n",
        "\n",
        "    train_set = TensorDataset(train_x, train_y)\n",
        "    train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
        "#     return torch.FloatTensor(x1), torch.FloatTensor(x2), torch.FloatTensor(target)\n",
        "    #return data\n",
        "    return train_loader\n",
        "       \n",
        "\n",
        "def plot_ndcg_loss(ndcgs):\n",
        "    x = np.arange(len(ndcgs))\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    ax.plot(x, ndcgs, label='NDCG')\n",
        "    ax.set_xlabel(\"Batch % 2000\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_title(\"Pointwise LTR\")\n",
        "    legend = ax.legend(loc='upper center')\n",
        "    \n",
        "    plt.show()\n",
        "    plt.savefig('Pairwise_LTR_plot.png')\n",
        "\n",
        "    \n",
        "def train_batch(documentfeatures, labels, model, sig):\n",
        "#     model.ranknet.train()\n",
        "    for epoch in range(1):\n",
        "#         for qid in range(0, train_data.num_queries()):\n",
        "#             if train_data.query_size(qid) < 2:\n",
        "#                 continue\n",
        " \n",
        "            model.optimizer.zero_grad()\n",
        "\n",
        "            output = model.ranknet(documentfeatures)\n",
        "            \n",
        "            loss = pairwiseloss(output, labels, sig)\n",
        "        \n",
        "            loss.backward()\n",
        "            \n",
        "            model.optimizer.step()\n",
        "            \n",
        "    return model\n",
        "    \n",
        "def pairwiseloss(predictedvals, values, sig):\n",
        "    \n",
        "    predictedvals = predictedvals.squeeze()\n",
        "\n",
        "    # Hier is waar er is gesleuteld aan de data om de nan's te voorkomen\n",
        "    if predictedvals.shape[0] == 0:\n",
        "        return torch.tensor([0.0], requires_grad= True)\n",
        "    # pairs = int(math.factorial(n_docs) / (math.factorial(n_docs - 2) * 2))\n",
        "    tups = list(itertools.combinations(range(predictedvals.shape[0]), 2))\n",
        "    \n",
        "\n",
        "    val1, val2 = [x[0] for x in tups], [x[1] for x in tups]\n",
        "    #todevice\n",
        "    pred1 = predictedvals[val1]\n",
        "    pred2 = predictedvals[val2]\n",
        "    \n",
        "    true1 = values[val1]\n",
        "    true2 = values[val2]\n",
        "    #.todevice\n",
        "    s1 = (true1 > true2).type(torch.FloatTensor)\n",
        "    s2 = (true1 < true2).type(torch.FloatTensor)\n",
        "    \n",
        "    S =  s1 - s2\n",
        "    S = torch.tensor(S)\n",
        "    sigma = sig\n",
        "    C_T = (0.5 * (1 - S) * sigma * (pred1-pred2) + torch.log(1 + torch.exp(-sigma*(pred1-pred2))))\n",
        "    C_T = torch.tensor(C_T, requires_grad = True)\n",
        "    \n",
        "\n",
        "    return C_T.mean()\n",
        "\n",
        "def hyperparam_search():\n",
        "    # hyper-parameters\n",
        "    epochs = 10\n",
        "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
        "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
        "#     learning_rates = [ 10**-1]\n",
        "#     n_hiddens = [150, 200, 250]\n",
        "    sig = [0.1, 1, 10, 100]\n",
        "    print(\"hi\")\n",
        "#     train_loader = load_dataset()\n",
        "    print(\"2\")\n",
        "    best_ndcg = 0\n",
        "    for learning_rate in learning_rates:\n",
        "        for n_hidden in n_hiddens:\n",
        "            for sigma in sig:\n",
        "\n",
        "                print(\"\\nTesting learning_rate = {}, n_hidden = {} and sigma = {}\".format(learning_rate, n_hidden, sigma))\n",
        "                model = Model(data.num_features, n_hidden, learning_rate, sigma)\n",
        "\n",
        "                last_ndcg = 0\n",
        "                for epoch in range(epochs):\n",
        "\n",
        "                    model.ranknet.train()\n",
        "                    for qid in range(0, data.train.num_queries()):#\n",
        "                        if data.train.query_size(qid) < 2:#\n",
        "                            continue#\n",
        "                        s_i, e_i = data.train.query_range(qid)\n",
        "\n",
        "                        documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
        "                        labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
        "    #                     if documentfeatures.shape[0] == 0:\n",
        "    #                         return torch.tensor([0.0], requires_grad= True)\n",
        "\n",
        "                        model = train_batch(documentfeatures, labels, model, sigma)  \n",
        "\n",
        "                                       \n",
        "                    scores = eval_model(model, data.validation)\n",
        "              \n",
        "                    ndcg = scores[\"ndcg\"][0]\n",
        "                    print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
        "                            \n",
        "                    if ndcg < last_ndcg:\n",
        "                        break\n",
        "                    last_ndcg = ndcg\n",
        "                    if ndcg > best_ndcg:\n",
        "                        best_ndcg = ndcg\n",
        "                        best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch, \"sigma\": sigma}            \n",
        "                        print(\"Best parameters:\", best_params)\n",
        "    \n",
        "    return best_params\n",
        "\n",
        "# hyperparam_search()\n",
        "\n",
        "\n",
        "\n",
        "#### Loss functie 1tje kiezen voor validation en train.\n",
        "####  \n",
        "\n",
        "# {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 8}\n",
        "# {'learning_rate': 0.1, 'n_hidden': 250, 'epoch': 6}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "veVi9--TlXx6",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_best(best_params):\n",
        "    epochs = best_params[\"epoch\"]\n",
        "    n_hidden = best_params[\"n_hidden\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    sigma = best_params[\"sigma\"]\n",
        "    \n",
        "    # load data\n",
        "#     data, train_loader = load_dataset()\n",
        "    model = Model(data.num_features, n_hidden, learning_rate, sigma)\n",
        "\n",
        "    losses, ndcgs = [], []\n",
        "    for epoch in range(epochs):\n",
        "        eval_count = 0\n",
        "        for qid in range(0, data.train.num_queries()):#\n",
        "            if data.train.query_size(qid) < 2:#\n",
        "                continue#\n",
        "            s_i, e_i = data.train.query_range(qid)\n",
        "            \n",
        "            documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
        "            labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
        "            model = train_batch(documentfeatures, labels, model, sigma) \n",
        "            eval_count +=1\n",
        "            if eval_count % 2000 == 0:\n",
        "                scores = eval_model(model, data.validation)\n",
        "                ndcgs.append(scores[\"ndcg\"][0])\n",
        "        print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
        "        \n",
        "    return ndcgs, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uy_km10Varfn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e43eb6e7-17ad-4b2b-b59d-e95e1316d808"
      },
      "source": [
        "\n",
        "# def train_best(best_params):\n",
        "#     epochs = best_params[\"epoch\"]\n",
        "#     n_hidden = best_params[\"n_hidden\"]\n",
        "#     learning_rate = best_params[\"learning_rate\"]\n",
        "    \n",
        "#     # load data\n",
        "#     data, train_loader = load_dataset()\n",
        "#     model = Model(data.num_features, n_hidden, learning_rate)\n",
        "\n",
        "#     losses, ndcgs = [], []\n",
        "#     for epoch in range(epochs):\n",
        "#         eval_count = 0\n",
        "#         for x_batch, y_batch in train_loader:\n",
        "#             model = train_batch(x_batch, y_batch, model)\n",
        "#             eval_count +=1\n",
        "#             if eval_count % 2000 == 0:\n",
        "#                 loss, scores = eval_model(model, data.validation)\n",
        "#                 losses.append(loss)\n",
        "#                 ndcgs.append(scores[\"ndcg\"][0])\n",
        "#         print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
        "        \n",
        "#     return ndcgs, losses, model\n",
        "\n",
        "\n",
        "def get_distributions(model):\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    model.ranknet.eval()\n",
        "\n",
        "    val_x = torch.from_numpy(data.validation.feature_matrix).float()\n",
        "    test_x = torch.from_numpy(data.test.feature_matrix).float()\n",
        "           \n",
        "    val = model.ranknet(val_x).detach().cpu().numpy().squeeze()\n",
        "    test = model.ranknet(test_x).detach().cpu().numpy().squeeze()\n",
        "    actual = np.concatenate((data.train.label_vector, data.validation.label_vector, data.test.label_vector))\n",
        "    \n",
        "    distributions = {\n",
        "    \"val_mean\": np.mean(val),\n",
        "    \"val_std\": np.std(val),\n",
        "    \"test_mean\": np.mean(test),\n",
        "    \"test_std\": np.std(test),\n",
        "    \"actual_mean\": np.mean(actual), \n",
        "    \"actual_std\": np.std(actual),\n",
        "    }\n",
        "    \n",
        "    return distributions\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #determine best hyper parameters\n",
        "    best_params = hyperparam_search()\n",
        "    #train best model\n",
        "    ndcgs, model = train_best(best_params)\n",
        "    #plot ndcg and loss    \n",
        "    plot_ndcg_loss(ndcgs)\n",
        "    #get distributions of scores\n",
        "    distributions = get_distributions(model)\n",
        "    #performance on test set\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    scores = eval_model(model, data.test)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "2\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 100 and sigma = 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7254304373723682\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 0, 'sigma': 0.1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7256076200855712\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 1, 'sigma': 0.1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7255075943152488\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 100 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6947550850784654\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6947602991265015\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6947859746863303\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.6948038542738088\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.6947796343069744\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 100 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7162216869439334\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7162436411250649\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7162301185423271\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 100 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6863091380156301\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6862677877804966\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 150 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.732879607053447\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 0, 'sigma': 0.1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7330395707114215\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 1, 'sigma': 0.1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.733128436045551\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 2, 'sigma': 0.1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7330016563838022\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 150 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7187915627820951\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7188007715171416\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7187896253645442\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 150 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7455209365701124\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 150, 'epoch': 0, 'sigma': 10}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7454855904942043\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 150 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6907045138511662\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6906528054930485\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 200 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7291742327484595\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7292699571168711\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7291090022973761\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 200 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7297838075887888\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.729838218933232\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7297795851956655\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 200 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7662548650121882\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 200, 'epoch': 0, 'sigma': 10}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.766183614987546\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 200 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7588495507504058\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7589126817530361\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7588390680501861\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 250 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7337945445093245\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7338815823339769\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7339163798136565\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7339123525445409\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 250 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7101703611658887\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.710046648935431\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 250 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7127525453872267\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7127266426984468\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 250 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.734654698173105\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7347627513417953\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.734702405662385\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 300 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7013400274257063\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7014211815679438\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7014386723226871\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.701579068740233\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.7014853640862364\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 300 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7418263078582121\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7418026780514945\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 300 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7133554525537436\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7134325758952139\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.713391544876791\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 300 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.71735908736341\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7174134911636032\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7174307891855964\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7174197474943764\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 350 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.696758604651878\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6967002207523743\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 350 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7123784215256189\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7122409185254496\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 350 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7706274225280049\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 350, 'epoch': 0, 'sigma': 10}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7706547329610026\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 350, 'epoch': 1, 'sigma': 10}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7705580588773785\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 350 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6880355943609333\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6880596684165387\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6880373627886335\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 400 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7244278820850698\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7242491257931044\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 400 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7359843877714252\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7360703939905993\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7360041990796963\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 400 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7239425770486161\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7237711951306579\n",
            "\n",
            "Testing learning_rate = 0.1, n_hidden = 400 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7119065143673182\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7118043005664032\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 100 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6929474879839044\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6929790153085582\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.693024347183274\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.6929701361742514\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 100 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7319383257122191\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7319408053307406\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.731957196831532\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7319137474608554\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 100 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7123174439184504\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7122544552656953\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 100 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7506661237946862\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7506532267097895\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 150 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7078355760144172\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7078634437946458\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7078776152724\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7078662283692465\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 150 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7020556216680229\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7022533649630887\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7020275494143835\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 150 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7310827323719692\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7311209752327891\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7310613825642157\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 150 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7287443560145588\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7287612166801349\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.728680976491042\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 200 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.690087219806549\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6901450049990001\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6902022641691802\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.6900887768587772\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 200 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6959655094953718\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6960398817908899\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6959132178729025\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 200 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7369397118600121\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7369908243175826\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7368117111570477\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 200 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7104985145902115\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7105503656952173\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7105693887922273\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7103644442237564\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 250 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6986644176576762\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6986706371056816\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.698610746984011\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 250 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7297061420014417\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7297554813690977\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7297733215230074\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7296552382512074\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 250 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7391618301800958\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.739071691960328\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 250 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6835399108221892\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6834486945476894\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 300 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7237457897173024\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7237189837185003\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 300 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7221321604052393\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.722066574234047\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 300 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7203247638550877\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7202955853863078\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 300 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7304222508400844\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7302745078774672\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 350 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6883047450436839\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6882146564596496\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 350 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7430092812157877\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7430318972267352\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7431133262229639\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7430331424850997\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 350 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7351325452438813\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7351973450079955\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7351798415718345\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 350 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7511558845398028\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.751343413345859\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.751156838976457\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 400 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6857720057964292\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6856561299873285\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 400 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7141936998236811\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7141831866143228\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 400 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.712326776030968\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7123492992983814\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7124220048607379\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7123670064919031\n",
            "\n",
            "Testing learning_rate = 0.01, n_hidden = 400 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7144555546327147\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7144667685836229\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7145660609782493\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7145020055694469\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 100 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6971223306354172\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6971461256562225\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6970663869369765\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 100 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7750705747430405\n",
            "Best parameters: {'learning_rate': 0.001, 'n_hidden': 100, 'epoch': 0, 'sigma': 1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7749477987372875\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 100 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.680929564541073\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6808872492701781\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 100 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7450204251144897\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7449825719618587\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 150 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7080668139181162\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7080910885109278\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7080398493430529\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 150 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7047280822303789\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7047641036926383\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7048373004066906\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7046936458114692\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 150 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7067456647619061\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7066569979300895\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 150 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7432995953395706\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7432237304899103\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 200 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7435565361920309\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7435872858234712\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7435768327073313\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 200 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7040644403406962\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7038856316426183\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 200 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7066551860496384\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7066741203792081\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7066618393665552\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 200 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6844664244919239\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6843790909834908\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 250 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7240266344915163\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7239087439195331\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 250 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6942322331289328\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6942572443398778\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6942490535679029\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 250 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7091771681752898\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7091861861389552\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7091159524217253\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 250 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7350455429125234\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7350697315606659\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7350408377165121\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 300 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7559384693522309\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7558339536883201\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 300 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7124101536819726\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.712451797195019\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7124136523157777\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 300 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7543068335115636\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7542875685422423\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 300 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7301325309487201\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7299452687352241\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 350 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7064501378823699\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7065016086691748\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7066487504770269\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7065494247681567\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 350 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7698408749784962\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7698076481169884\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 350 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6819120601586152\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6819676413988879\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6819838101374703\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.6818718704918821\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 350 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6795057021787093\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6795652912067899\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6795049848437371\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 400 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7037794746778726\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7038067735108098\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7037667250374096\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 400 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.759477659970474\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7593964183601408\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 400 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6907048048416672\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6907148342357087\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6906143867525601\n",
            "\n",
            "Testing learning_rate = 0.001, n_hidden = 400 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6963281830119561\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6963392638775917\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6963537709661044\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.696346342336086\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 100 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.74915009933671\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7492219113588444\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7491685367809507\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 100 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7214805679799592\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7213952408937134\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 100 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6975784114567372\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6975633221767004\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 100 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6981124993094511\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.698175978878128\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.698107397204405\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 150 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.69636359832595\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6962431525792386\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 150 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.712641058134633\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.712722857430588\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7125528670694806\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 150 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7053596744156599\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7053465988075311\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 150 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7147635525536233\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7147958732557971\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.714695055157754\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 200 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7141989565391295\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7142086819049285\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.714237849420161\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7141665722508614\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 200 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7309194872586524\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7309035854877002\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 200 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7156938606393513\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7156195911157506\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 200 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7218582858508059\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7218122406580036\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 250 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6919889348910029\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6919928308704545\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6919906857580068\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 250 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7268625789612686\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.726847367555837\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 250 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7343072568409184\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7341454477901601\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 250 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7474850854161907\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7474703482299905\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 300 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7239641740817022\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7238896181239844\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 300 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7287343135657296\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7287495885516811\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.728856293722009\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7288666834579749\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.7287257732516471\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 300 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7401715205788901\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7402334657496258\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7401583444595199\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 300 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7236485312992968\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7236210797242499\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 350 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7038044192801076\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7039007884677242\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7039226902328178\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7039091618210949\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 350 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7148812051315233\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7148915699297601\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7149305391764549\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7148881677711442\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 350 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7505022533015968\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7506950507214598\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7505912495083793\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 350 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7146699660437189\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7145873206624281\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 400 and sigma = 0.1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7060292433576119\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7060194994579686\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 400 and sigma = 1\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6900612977402771\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6900664822899644\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.6900377307108427\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 400 and sigma = 10\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.6962312430013622\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.6963111959865022\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.696286901744974\n",
            "\n",
            "Testing learning_rate = 0.0001, n_hidden = 400 and sigma = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7149090043424136\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7149120928042291\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 2, ndcg: 0.7149645880445316\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 3, ndcg: 0.7149653399364381\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 4, ndcg: 0.7149074793536999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\"metric\": \"mean\" (\"standard deviation\")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PYr5Lqm6ausr",
        "outputId": "153df05d-46a1-42fc-b2b4-72c80bcebf70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#torch.manual_seed(0)\n",
        "\n",
        "class RankNetSU(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden):\n",
        "        super(RankNetSU, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.output = torch.nn.Linear(n_hidden, 1)      \n",
        "        \n",
        "    def forward(self, x1):\n",
        "        x = torch.nn.functional.relu(self.hidden(x1))\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, n_feature, n_hidden, learning_rate, sigma):\n",
        "        self.ranknet = RankNetSU(n_feature, n_hidden)\n",
        "        #.to(device)\n",
        "        self.optimizer = torch.optim.SGD(self.ranknet.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_model(model, data_fold):\n",
        "    #with torch.no_grad():\n",
        "        x = torch.from_numpy(data_fold.feature_matrix).float()\n",
        "        y = data_fold.label_vector\n",
        "        model.ranknet.eval()\n",
        "               \n",
        "        output = model.ranknet(x)\n",
        "        output = output.detach().cpu().numpy().squeeze()\n",
        "        \n",
        "        #loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-y)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
        "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "\n",
        "    train_x = torch.from_numpy(data.train.feature_matrix).float()\n",
        "    train_y = torch.from_numpy(data.train.label_vector).float()\n",
        "\n",
        "    train_set = TensorDataset(train_x, train_y)\n",
        "    train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "    return train_loader\n",
        "       \n",
        "\n",
        "def plot_ndcg_loss(losses, ndcgs):\n",
        "    x = np.arange(len(losses))\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    ax.plot(x, losses, label='Loss')\n",
        "    ax.plot(x, ndcgs, label='NDCG')\n",
        "    ax.set_xlabel(\"Batch % 2000\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_title(\"Pointwise LTR\")\n",
        "    legend = ax.legend(loc='upper center')\n",
        "    \n",
        "    plt.show()\n",
        "    plt.savefig('Pairwise_LTR_plot.png')\n",
        "\n",
        "    \n",
        "def train_batch(documentfeatures, labels, model, sig):\n",
        "#     model.ranknet.train()\n",
        "    for epoch in range(1):\n",
        "#         for qid in range(0, train_data.num_queries()):\n",
        "#             if train_data.query_size(qid) < 2:\n",
        "#                 continue\n",
        "            model.optimizer.zero_grad()\n",
        "\n",
        "            output = model.ranknet(documentfeatures)\n",
        "            \n",
        "            loss = pairwiseloss(output, labels, sig)\n",
        "            \n",
        "            # AttributeError: 'Tensor' object has no attribute 'forward'\n",
        "            \n",
        "            loss.sum().backward()\n",
        "            \n",
        "            model.optimizer.step()\n",
        "\n",
        "    return model\n",
        "    \n",
        "def pairwiseloss(predictedvals, values, sig):\n",
        "    \n",
        "    predictedvals = predictedvals.squeeze()\n",
        "\n",
        "    if predictedvals.shape[0] == 0:\n",
        "        return torch.tensor([0.0], requires_grad= True)\n",
        "    tups = list(itertools.combinations(range(predictedvals.shape[0]), 2))\n",
        "#     print(tups)\n",
        "    val1, val2 = [x[0] for x in tups], [x[1] for x in tups]\n",
        "    #todevice\n",
        "    pred1 = predictedvals[val1]\n",
        "    pred2 = predictedvals[val2]\n",
        "    \n",
        "    true1 = values[val1]\n",
        "    true2 = values[val2]\n",
        "    #.todevice\n",
        "    s1 = (true1 > true2).type(torch.FloatTensor)\n",
        "    s2 = (true1 < true2).type(torch.FloatTensor)\n",
        "    \n",
        "    S =  s1 - s2\n",
        "    S = torch.tensor(S)\n",
        "#     print(S)\n",
        "#     print(s1)\n",
        "#     print(s2)\n",
        "    sigma = sig\n",
        "#     print(val1)\n",
        "    \n",
        "    lambda_ij = sig*(torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+torch.exp((pred1-pred2)*sigma))))\n",
        "#     print(lambda_ij.shape)\n",
        "#     lambda_ij = torch.sigmoid(torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S)torch.log(torch.FloatTensor([1])+torch.exp(-((s_i-s_j).sigmoid().view(s_i.size(0)))))\n",
        "#     print(predictedvals.shape)\n",
        "    lambda_i = np.zeros(len(set(val1))+1)\n",
        "    n=0\n",
        "    for i in val1:\n",
        "        lambda_i[i]+= lambda_ij[n]\n",
        "        n += 1\n",
        "#     print(val2)\n",
        "    m = 0\n",
        "    for j in val2:\n",
        "        lambda_i[j]-= lambda_ij[m] \n",
        "        m += 1\n",
        "    lambda_i = torch.tensor(lambda_i, requires_grad=True)\n",
        "#     print(lambda_i)\n",
        "#     print(lambda_i.shape)\n",
        "    return predictedvals * lambda_i.detach()\n",
        "\n",
        "def hyperparam_search():\n",
        "    # hyper-parameters\n",
        "    epochs = 2\n",
        "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
        "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
        "    sigma = [-1, 1, 10, 60, 100]\n",
        "    # learning_rates = [10**-2]\n",
        "    # n_hiddens = [100]\n",
        "    # sigma = [1]\n",
        "    print(\"hi\")\n",
        "#     train_loader = load_dataset()\n",
        "    print(\"2\")\n",
        "    best_ndcg = 0\n",
        "    for learning_rate in learning_rates:\n",
        "        for n_hidden in n_hiddens:\n",
        "            for sig in sigma:\n",
        "        \n",
        "                print(\"\\nTesting learning_rate = {} and n_hidden = {}\".format(learning_rate, n_hidden))\n",
        "                model = Model(data.num_features, n_hidden, learning_rate, sig)\n",
        "\n",
        "                last_ndcg = 0\n",
        "                for epoch in range(epochs):\n",
        "\n",
        "                    model.ranknet.train()\n",
        "                    for qid in range(0, data.train.num_queries()):#\n",
        "                        if data.train.query_size(qid) < 2:#\n",
        "                            continue#\n",
        "                        s_i, e_i = data.train.query_range(qid)\n",
        "\n",
        "                        documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
        "                        labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
        "    #                     if documentfeatures.shape[0] == 0:\n",
        "    #                         return torch.tensor([0.0], requires_grad= True)\n",
        "\n",
        "                        model = train_batch(documentfeatures, labels, model, sig)  \n",
        "\n",
        "\n",
        "                    scores = eval_model(model, data.validation)\n",
        "\n",
        "                    ndcg = scores[\"ndcg\"][0]\n",
        "                    print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
        "\n",
        "                    if ndcg < last_ndcg:\n",
        "                        break\n",
        "                    last_ndcg = ndcg\n",
        "                    if ndcg > best_ndcg:\n",
        "                        best_ndcg = ndcg\n",
        "                        best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch, \"sigma\": sig}            \n",
        "                        print(\"Best parameters:\", best_params)\n",
        "    \n",
        "    return best_params\n",
        "\n",
        "# hyperparam_search()\n",
        "\n",
        "\n",
        "#### Loss functie 1tje kiezen voor validation en train.\n",
        "####  \n",
        "\n",
        "# {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 8}\n",
        "def train_best(best_params):\n",
        "    epochs = best_params[\"epoch\"]\n",
        "    n_hidden = best_params[\"n_hidden\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    sigma = best_params[\"sigma\"]\n",
        "    \n",
        "    # load data\n",
        "#     data, train_loader = load_dataset()\n",
        "    model = Model(data.num_features, n_hidden, learning_rate, sigma)\n",
        "\n",
        "    losses, ndcgs = [], []\n",
        "    for epoch in range(epochs):\n",
        "        eval_count = 0\n",
        "        for qid in range(0, data.train.num_queries()):#\n",
        "            if data.train.query_size(qid) < 2:#\n",
        "                continue#\n",
        "            s_i, e_i = data.train.query_range(qid)\n",
        "            \n",
        "            documentfeatures = torch.tensor(data.train.feature_matrix[s_i:e_i]).float()\n",
        "            labels = torch.tensor(data.train.label_vector[s_i:e_i])\n",
        "            model = train_batch(documentfeatures, labels, model, sigma) \n",
        "            eval_count +=1\n",
        "            if eval_count % 2000 == 0:\n",
        "                scores = eval_model(model, data.validation)\n",
        "                ndcgs.append(scores[\"ndcg\"][0])\n",
        "        print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
        "        \n",
        "    return ndcgs, model\n",
        "\n",
        "\n",
        "def get_distributions(model):\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    model.ranknet.eval()\n",
        "\n",
        "    val_x = torch.from_numpy(data.validation.feature_matrix).float()\n",
        "    test_x = torch.from_numpy(data.test.feature_matrix).float()\n",
        "           \n",
        "    val = model.ranknet(val_x).detach().cpu().numpy().squeeze()\n",
        "    test = model.ranknet(test_x).detach().cpu().numpy().squeeze()\n",
        "    actual = np.concatenate((data.train.label_vector, data.validation.label_vector, data.test.label_vector))\n",
        "    \n",
        "    distributions = {\n",
        "    \"val_mean\": np.mean(val),\n",
        "    \"val_std\": np.std(val),\n",
        "    \"test_mean\": np.mean(test),\n",
        "    \"test_std\": np.std(test),\n",
        "    \"actual_mean\": np.mean(actual), \n",
        "    \"actual_std\": np.std(actual),\n",
        "    }\n",
        "    \n",
        "    return distributions\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #determine best hyper parameters\n",
        "    best_params = hyperparam_search()\n",
        "    #train best model\n",
        "    ndcgs, model = train_best(best_params)\n",
        "    #plot ndcg and loss    \n",
        "    plot_ndcg_loss(ndcgs)\n",
        "    #get distributions of scores\n",
        "    distributions = get_distributions(model)\n",
        "    #performance on test set\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    scores = eval_model(model, data.test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "2\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7190389939666768\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 0, 'sigma': -1}\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7201143979826179\n",
            "Best parameters: {'learning_rate': 0.1, 'n_hidden': 100, 'epoch': 1, 'sigma': -1}\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.720002235871374\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7195722413730479\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7171255579091167\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7173110698553419\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.717013554564625\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7193938019733462\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 100\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.717357090117302\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7165760278768779\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 150\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7152042968345559\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7157589635108316\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 150\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7197241412133751\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7147915533079301\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 150\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7183405982297991\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7185537548693088\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 150\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7171606259084929\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7180482819350278\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 150\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7175608610854971\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7185994688116293\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 200\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7180478437876985\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.717895367917309\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 200\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7170084393285331\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 1, ndcg: 0.7156935712330622\n",
            "\n",
            "Testing learning_rate = 0.1 and n_hidden = 200\n",
            "\"metric\": \"mean\" (\"standard deviation\")\n",
            "Epoch: 0, ndcg: 0.7192735251122877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6wv0Oqd4akqP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nNNyCVQ5akqa",
        "colab": {}
      },
      "source": [
        "## Below is Rubens code altered to include a sigma instead of a sigmoid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjIBv_tGUAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dataset\n",
        "import ranking as rnk\n",
        "import evaluate as evl\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "from scipy import stats\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "data = dataset.get_dataset().get_data_folds()[0]\n",
        "data.read_data()\n",
        "\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, n_feature, n_hidden, learning_rate, sigma):\n",
        "        self.net = nn.Sequential(\n",
        "                                 nn.Linear(n_feature, n_hidden),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Linear(n_hidden, 1)\n",
        "                                )\n",
        "        self.criterion = pair_loss\n",
        "        self.optimizer = torch.optim.SGD(self.net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "def pair_loss(output, y, sigma):\n",
        "    output = output.squeeze()\n",
        "    \n",
        "    if y.shape[0] == 1:\n",
        "        return (output - y)**2\n",
        "\n",
        "    pairs = list(itertools.combinations(range(output.shape[0]), 2))\n",
        "    val1, val2 = [x[0] for x in pairs], [x[1] for x in pairs]\n",
        "\n",
        "    # pred1 = output[val1].to(device)\n",
        "    # pred2 = output[val2].to(device)\n",
        "    pred1 = output[val1]\n",
        "    pred2 = output[val2]\n",
        "\n",
        "    true1 = y[val1]\n",
        "    true2 = y[val2]\n",
        "\n",
        "    # s1 = (true1 > true2).type(torch.ByteTensor).to(device)\n",
        "    # s2 = (true1 < true2).type(torch.ByteTensor).to(device)\n",
        "    s1 = (true1 > true2).type(torch.ByteTensor)\n",
        "    s2 = (true1 < true2).type(torch.ByteTensor)\n",
        "\n",
        "    S =  s1 - s2\n",
        "    # sigmoid = torch.sigmoid(pred1.float() - pred2.float())\n",
        "    \n",
        "    C = (0.5 * (1 - S) * sigma + torch.log(1 + torch.exp(-sigma*(pred1-pred2))))\n",
        "    \n",
        "    return C.mean()     \n",
        "    \n",
        "\n",
        "def eval_model(model, data_fold):\n",
        "    with torch.no_grad():\n",
        "        x = torch.from_numpy(data_fold.feature_matrix).float()\n",
        "        y = data_fold.label_vector\n",
        "        model.net.eval()\n",
        "               \n",
        "        output = model.net(x)      \n",
        "          \n",
        "        output = output.detach().cpu().numpy().squeeze()\n",
        "        \n",
        "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
        "\n",
        "    return scores\n",
        "\n",
        "def calc_ERR(model, data_fold):\n",
        "    ERR = 0\n",
        "    for qid in range(data_fold.num_queries()):\n",
        "                    \n",
        "        s_i, e_i = data.train.query_range(qid)\n",
        "        x = torch.from_numpy(data.train.feature_matrix[s_i:e_i]).float()\n",
        "        y = torch.from_numpy(data.train.label_vector[s_i:e_i])\n",
        "        \n",
        "        output = model.net(x)\n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "def train_batch(x_batch, y_batch, model, sigma):\n",
        "    model.net.train()\n",
        "    model.optimizer.zero_grad()\n",
        "    # x_batch = x_batch.to(device)\n",
        "    # y_batch = y_batch.to(device) \n",
        "    x_batch = x_batch\n",
        "    y_batch = y_batch\n",
        "           \n",
        "    output = model.net(x_batch)\n",
        "    loss = model.criterion(output, y_batch, sigma)\n",
        "    \n",
        "    loss.backward()\n",
        "    model.optimizer.step()\n",
        "    \n",
        "    return model\n",
        "\n",
        "       \n",
        "def hyperparam_search():\n",
        "    # hyper-parameters\n",
        "    epochs = 300\n",
        "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
        "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
        "    sigmas = [1, 10, 60, 100]\n",
        "\n",
        "    \n",
        "    best_ndcg = 0\n",
        "    for learning_rate in learning_rates:\n",
        "        for n_hidden in n_hiddens:\n",
        "          for sigma in sigmas:\n",
        "        \n",
        "              print(\"\\nTesting learning_rate = {}, n_hidden = {} and sigma = {}\".format(learning_rate, n_hidden, sigma))\n",
        "              model = Model(data.num_features, n_hidden, learning_rate, sigma)\n",
        "              switch = False\n",
        "              last_ndcg = 0\n",
        "              for epoch in range(epochs):\n",
        "                  \n",
        "                  model.net.train()\n",
        "                  for qid in range(data.train.num_queries()):\n",
        "                      \n",
        "                      s_i, e_i = data.train.query_range(qid)\n",
        "                      x_batch = torch.from_numpy(data.train.feature_matrix[s_i:e_i]).float()\n",
        "                      y_batch = torch.from_numpy(data.train.label_vector[s_i:e_i])\n",
        "\n",
        "                      model = train_batch(x_batch, y_batch, model, sigma)  \n",
        "                  \n",
        "                  scores = eval_model(model, data.validation)                                    \n",
        "                  ndcg = scores[\"ndcg\"][0]\n",
        "                              \n",
        "                  if ndcg < last_ndcg:\n",
        "                      if switch: break\n",
        "                      switch = True\n",
        "                  \n",
        "                  last_ndcg = ndcg\n",
        "                  if ndcg > best_ndcg:\n",
        "                      best_ndcg = ndcg\n",
        "                      best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch, \"sigma\": sigma}            \n",
        "                      print(\"Best parameters:\", best_params)\n",
        "                  print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
        "    \n",
        "    return best_params\n",
        "    \n",
        "    \n",
        "def train_best(best_params):\n",
        "    epochs = best_params[\"epoch\"]\n",
        "    n_hidden = best_params[\"n_hidden\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    sigma = best_params[\"sigma\"]\n",
        "    \n",
        "    model = Model(data.num_features, n_hidden, learning_rate, sigma)\n",
        "\n",
        "    ndcgs = []\n",
        "    for epoch in range(epochs):\n",
        "        eval_count = 0\n",
        "        for qid in range(data.train.num_queries()):\n",
        "                    \n",
        "            s_i, e_i = data.train.query_range(qid)\n",
        "            x_batch = torch.from_numpy(data.train.feature_matrix[s_i:e_i]).float()\n",
        "            y_batch = torch.from_numpy(data.train.label_vector[s_i:e_i])\n",
        "\n",
        "            model = train_batch(x_batch, y_batch, model, sigma)\n",
        "            \n",
        "            eval_count +=1\n",
        "            if eval_count % 100 == 0:\n",
        "                scores = eval_model(model, data.validation)\n",
        "                ndcgs.append(scores[\"ndcg\"][0])\n",
        "        \n",
        "        print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
        "        \n",
        "    return ndcgs, model\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #determine best hyper parameters\n",
        "    best_params = hyperparam_search()\n",
        "    #train best model\n",
        "    ndcgs, losses, model = train_best(best_params)\n",
        "    #performance on test set\n",
        "    scores = eval_model(model, data.test)\n",
        "    error = calc_err(model, data.test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-VcTZwpGTbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWHKj8cQGTJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c4jxUhn8akqn",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}