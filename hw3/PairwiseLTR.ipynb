{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "PairwiseLTR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhNC8yFuD2eo",
        "colab_type": "code",
        "outputId": "7a08aa83-273b-4287-bb34-c93ee50f4edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "% cd drive/My Drive/hw3/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/hw3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAbI82HvEJgL",
        "colab_type": "code",
        "outputId": "a1fad6c4-18db-4a54-d1a0-6e721357f02d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset\t\t\t\t       HW3-output.ipynb   ranking_example.py\n",
            "datasetbinarized_purged_querynorm.npz  ir1_2020_hw3.pdf   ranking.py\n",
            "dataset.py\t\t\t       PairwiseLTR.ipynb  try\n",
            "evaluate.py\t\t\t       pointwise_LTR.py   untitled\n",
            "example.py\t\t\t       __pycache__\t  Untitled.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILMI8Kmhakp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dataset\n",
        "import ranking as rnk\n",
        "import evaluate as evl\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class RankNet(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.output = torch.nn.Linear(n_hidden, 1)      \n",
        "        \n",
        "    def forward(self, x1):\n",
        "        x = torch.nn.functional.relu(self.hidden(x1))\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, n_feature, n_hidden, learning_rate):\n",
        "        self.ranknet = RankNet(n_feature, n_hidden).to(device)\n",
        "        self.optimizer = torch.optim.SGD(self.ranknet.parameters(), lr=learning_rate)\n",
        "\n",
        "def eval_model(model, data_fold):\n",
        "    with torch.no_grad():\n",
        "        x = torch.from_numpy(data_fold.feature_matrix).float().to(device)\n",
        "        y = data_fold.label_vector\n",
        "        model.ranknet.eval()\n",
        "               \n",
        "        output = model.ranknet(x)\n",
        "        output = output.detach().cpu().numpy().squeeze()\n",
        "        \n",
        "        loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-y)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
        "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
        "\n",
        "    return loss, scores\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "\n",
        "#     train_x = torch.from_numpy(data.train.feature_matrix).float()\n",
        "#     train_y = torch.from_numpy(data.train.label_vector).float()\n",
        "\n",
        "    documents = data.train.feature_matrix\n",
        "    doc_list = list(range(len(documents)))\n",
        "    \n",
        "    # Carthesian product\n",
        "    Carth = list(itertools.combinations(doc_list,2))\n",
        "    x1, x2, target = [], [], []\n",
        "  \n",
        "    # iterate over all possible combinations\n",
        "    for i,j in Carth:\n",
        "        x1.append(docs[i])\n",
        "        x2.append(docs[j])\n",
        "        if data.train.label_vector[i]>data.train.label_vector[j]:\n",
        "            # this is the S_{ij}\n",
        "            target.append(float(1))\n",
        "        elif data.train.label_vector[i]<data.train.label_vector[j]:\n",
        "            target.append(float(-1))\n",
        "        else:\n",
        "            target.append(float(0))\n",
        "    train_set = TensorDataset(x1, x2, target)\n",
        "    train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
        "    \n",
        "#     return torch.FloatTensor(x1), torch.FloatTensor(x2), torch.FloatTensor(target)\n",
        "\n",
        "    return data, train_loader\n",
        "       \n",
        "\n",
        "def plot_ndcg_loss(losses, ndcgs):\n",
        "    x = np.arange(len(losses))\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    ax.plot(x, losses, label='Loss')\n",
        "    ax.plot(x, ndcgs, label='NDCG')\n",
        "    ax.set_xlabel(\"Batch % 2000\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_title(\"Pointwise LTR\")\n",
        "    legend = ax.legend(loc='upper center')\n",
        "    \n",
        "    plt.show()\n",
        "    plt.savefig('Pointwise_LTR_plot.png')\n",
        "\n",
        "    \n",
        "def train_batch(train_data, y_batch, model):\n",
        "    model.ranknet.train()\n",
        "    for epoch in range(1):\n",
        "        for qid in range(0, train_data.num_queries(qid)):\n",
        "            if train_data.query_size(qid) < 2:\n",
        "                continue\n",
        "                \n",
        "            s_i, e_i = train_data.query_range(qid)\n",
        "            \n",
        "            documentfeatures = torch.tensor(train_data.feature_matrix[s_i:e_i]).float()\n",
        "            labels = torch.tensor(train_data.label_vector[s_i:e_i])\n",
        "            \n",
        "    #x_batch = x_batch.to(device)\n",
        "    #y_batch = y_batch.to(device)\n",
        "           \n",
        "            output = model.ranknet(documentfeatures)\n",
        "            \n",
        "            loss = pairwiseloss(output, labels)\n",
        "            loss.backward()\n",
        "            model.optimizer.step()\n",
        "            model.optimizer.zero_grad() \n",
        "            \n",
        "        \n",
        "    return model\n",
        "    \n",
        "def pairwiseloss(predictedvals, values, n_docs):\n",
        "    values_true = values / 4\n",
        "    predictedvals = predictedvals.squeeze()\n",
        "    pairs = int(math.factorial(n_docs) / (math.factorial(n_docs - 2) * 2))\n",
        "    \n",
        "    tups = list(itertools.combinations(range(predictedvals.shape[0]), 2))\n",
        "    val1, val2 = [x[0] for x in tups], [x[1] for x in tups]\n",
        "    pred1 = predictedvals[val1].to(device)\n",
        "    pred2 = predictedvals[val2].to(device)\n",
        "    \n",
        "    true1 = values[val1].to(device)\n",
        "    true2 = values[val2].to(device)\n",
        "    \n",
        "    l1 = (true1 > true2).type(torch.ByteTensor).to(device)\n",
        "    l2 = (true1 < true2).type(torch.ByteTensor).to(device)\n",
        "    S = torch.zeros(pairs).to(device) + l1 - l2\n",
        "    \n",
        "    sigmoid = torch.sigmoid(pred1.float() - pred2.float()).to(device)\n",
        "    C_T = (0.5 * (1 - S) * sigmoid + torch.log(1 + torch.exp(-sigmoid)))\n",
        "    \n",
        "    return C_T.mean()\n",
        "\n",
        "def hyperparam_search():\n",
        "    # hyper-parameters\n",
        "    epochs = 300\n",
        "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
        "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
        "    data, train_loader = load_dataset()\n",
        "    \n",
        "    best_ndcg = 0\n",
        "    for learning_rate in learning_rates:\n",
        "        for n_hidden in n_hiddens:\n",
        "        \n",
        "            print(\"\\nTesting learning_rate = {} and n_hidden = {}\".format(learning_rate, n_hidden))\n",
        "            model = Model(data.num_features, n_hidden, learning_rate)\n",
        "            \n",
        "            last_ndcg = 0\n",
        "            for epoch in range(epochs):\n",
        "                \n",
        "                model.ranknet.train()\n",
        "                for x_batch, y_batch in train_loader:\n",
        "                    model = train_batch(x_batch, y_batch, model)                          \n",
        "                loss, scores = eval_model(model, data.validation)\n",
        "                \n",
        "                ndcg = scores[\"ndcg\"][0]\n",
        "                print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
        "                            \n",
        "                if ndcg < last_ndcg:\n",
        "                    break\n",
        "                last_ndcg = ndcg\n",
        "                if ndcg > best_ndcg:\n",
        "                    best_ndcg = ndcg\n",
        "                    best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch}            \n",
        "                    print(\"Best parameters:\", best_params)\n",
        "    \n",
        "    return best_params\n",
        "    \n",
        "    \n",
        "def train_best(best_params):\n",
        "    epochs = best_params[\"epoch\"]\n",
        "    n_hidden = best_params[\"n_hidden\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    \n",
        "    # load data\n",
        "    data, train_loader = load_dataset()\n",
        "    model = Model(data.num_features, n_hidden, learning_rate)\n",
        "\n",
        "    losses, ndcgs = [], []\n",
        "    for epoch in range(epochs):\n",
        "        eval_count = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            model = train_batch(x_batch, y_batch, model)\n",
        "            eval_count +=1\n",
        "            if eval_count % 2000 == 0:\n",
        "                loss, scores = eval_model(model, data.validation)\n",
        "                losses.append(loss)\n",
        "                ndcgs.append(scores[\"ndcg\"][0])\n",
        "        print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
        "        \n",
        "    return ndcgs, losses, model\n",
        "\n",
        "\n",
        "def get_distributions(model):\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    model.ranknet.eval()\n",
        "\n",
        "    val_x = torch.from_numpy(data.validation.feature_matrix).float().to(device)\n",
        "    test_x = torch.from_numpy(data.test.feature_matrix).float().to(device)\n",
        "           \n",
        "    val = model.ranknet(val_x).detach().cpu().numpy().squeeze()\n",
        "    test = model.ranknet(test_x).detach().cpu().numpy().squeeze()\n",
        "    actual = np.concatenate((data.train.label_vector, data.validation.label_vector, data.test.label_vector))\n",
        "    \n",
        "    distributions = {\n",
        "    \"val_mean\": np.mean(val),\n",
        "    \"val_std\": np.std(val),\n",
        "    \"test_mean\": np.mean(test),\n",
        "    \"test_std\": np.std(test),\n",
        "    \"actual_mean\": np.mean(actual), \n",
        "    \"actual_std\": np.std(actual),\n",
        "    }\n",
        "    \n",
        "    return distributions\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #determine best hyper parameters\n",
        "    best_params = hyperparam_search()\n",
        "    #train best model\n",
        "    ndcgs, losses, model = train_best(best_params)\n",
        "    #plot ndcg and loss    \n",
        "    plot_ndcg_loss(losses, ndcgs)\n",
        "    #get distributions of scores\n",
        "    distributions = get_distributions(model)\n",
        "    #performance on test set\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    loss, scores = eval_model(model, data.test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy_km10Varfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install evaluate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYr5Lqm6ausr",
        "colab_type": "code",
        "outputId": "f08fb301-0be3-42b1-cea7-1a4f270bf7c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "\n",
        "!ls\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/MyDrive/hw3')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin\t\t\t\t\t   etc\t  opt\t sys\n",
            "boot\t\t\t\t\t   home   proc\t tensorflow-2.1.0\n",
            "content\t\t\t\t\t   lib\t  root\t tmp\n",
            "datalab\t\t\t\t\t   lib32  run\t tools\n",
            "dev\t\t\t\t\t   lib64  sbin\t usr\n",
            "dlib-19.18.0-cp27-cp27mu-linux_x86_64.whl  media  srv\t var\n",
            "dlib-19.18.0-cp36-cp36m-linux_x86_64.whl   mnt\t  swift\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wv0Oqd4akqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dataset\n",
        "import ranking as rnk\n",
        "import evaluate as evl\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class RankNet(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.output = torch.nn.Linear(n_hidden, 1)      \n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        si = torch.nn.functional.relu(self.hidden(x1))\n",
        "        si = self.output(si)\n",
        "        sj = torch.nn.functional.relu(self.hidden(x2))\n",
        "        sj = self.output(sj)\n",
        "        return si, sj\n",
        "\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, n_feature, n_hidden, n_q, learning_rate):\n",
        "        for query_id in n_q:\n",
        "            start_index, end_index = data.train.query_range(query_id)\n",
        "            \n",
        "            x1, x2, target = self.target_feature(start_index, end_index)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # maybe another way\n",
        "            try:\n",
        "                si, sj = RankNet.forward(x1, x2)\n",
        "            except:\n",
        "                print(\"Query with one document, skipping this query\", qid)\n",
        "                continue\n",
        "                \n",
        "            # Get pairs\n",
        "            loss = torch.FloatTensor([0.5]) * (torch.FloatTensor([1]) - target) * ((si - sj).sigmoid()).view(\n",
        "                si.size(0)) + torch.log(torch.FloatTensor([1]) + torch.exp(-((si - sj).sigmoid().view(si.size(0)))))\n",
        "\n",
        "            loss.sum().backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if not query_id % 100:\n",
        "                print(f\"Data: {query_id}, with a loss of {loss.sum()}\")\n",
        "                all_scores = self.model(torch.FloatTensor(data.validation.feature_matrix)).view(-1).detach().numpy()\n",
        "                metrics = self.evaluate(data.validation, all_scores)\n",
        "                ndcg = metrics['ndcg']\n",
        "                arr = metrics['relevant rank']\n",
        "                if self.use_early_stop:\n",
        "                    self.normal_ndcg.append(ndcg[0])\n",
        "                    self.normal_ndcg.pop(0)\n",
        "                    x = list(range(0, len(self.normal_ndcg)))\n",
        "                    slope, *_ = stats.linregress(x, self.normal_ndcg)\n",
        "                    if slope < 0:\n",
        "                        print(f\"negative slope, early stopping activated slope of: {slope}\")\n",
        "                        break\n",
        "\n",
        "                print(f\"the ndcg is {ndcg} the arr is {arr}\")\n",
        "\n",
        "        all_scores = model.model(torch.FloatTensor(data.train.feature_matrix)).view(-1).detach().numpy()\n",
        "        metrics = model.evaluate(data.train, all_scores)\n",
        "        ndcg = metrics['ndcg']\n",
        "        print(f\"Train ndcg is {ndcg}\")\n",
        "\n",
        "#         self.net = Net(n_feature, n_hidden).to(device)\n",
        "#         self.criterion = torch.nn.MSELoss(reduction='mean')\n",
        "#         self.optimizer = torch.optim.SGD(self.net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "def eval_model(model, data_fold):\n",
        "    with torch.no_grad():\n",
        "        x = torch.from_numpy(data_fold.feature_matrix).float().to(device)\n",
        "        y = data_fold.label_vector\n",
        "        model.net.eval()\n",
        "               \n",
        "        output = model.net(x)\n",
        "        output = output.detach().cpu().numpy().squeeze()\n",
        "        \n",
        "        loss = np.mean(np.square(output - y))\n",
        "        scores = evl.evaluate(data_fold, np.asarray(output))  \n",
        "\n",
        "    return loss, scores\n",
        "\n",
        "\n",
        "# def load_dataset():\n",
        "#     data = dataset.get_dataset().get_data_folds()[0]\n",
        "#     data.read_data()\n",
        "\n",
        "#     train_x = torch.from_numpy(data.train.feature_matrix).float()\n",
        "#     train_y = torch.from_numpy(data.train.label_vector).float()\n",
        "\n",
        "#     train_set = TensorDataset(train_x, train_y)\n",
        "#     train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "#     return data, train_loader\n",
        "       \n",
        "    \n",
        "def target_feature(self, start_index, end_index):\n",
        "        documents = data.train.feature_matrix[start_index:end_index]\n",
        "        document_list = list(range(len(docs)))\n",
        "\n",
        "        # Carthesian product of all documents\n",
        "        Carth = list(itertools.combinations(document_list, 2))\n",
        "        x1, x2, target = [], [], []\n",
        "\n",
        "        # iterate over all possible combinations\n",
        "        for i, j in Cart:\n",
        "            x1.append(documents[i])\n",
        "            x2.append(documents[j])\n",
        "            if data.train.label_vector[i + start_index] > data.train.label_vector[j + start_index]:\n",
        "                #  S_ij\n",
        "                target.append(float(1))\n",
        "            elif data.train.label_vector[i + start_index] < data.train.label_vector[j + start_index]:\n",
        "                target.append(float(-1))\n",
        "            else:\n",
        "                target.append(float(0))\n",
        "        return torch.FloatTensor(input_1), torch.FloatTensor(input_2), torch.FloatTensor(target)\n",
        "\n",
        "\n",
        "def plot_ndcg_loss(losses, ndcgs):\n",
        "    x = np.arange(len(losses))\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    ax.plot(x, losses, label='Loss')\n",
        "    ax.plot(x, ndcgs, label='NDCG')\n",
        "    ax.set_xlabel(\"Batch % 2000\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_title(\"Pointwise LTR\")\n",
        "    legend = ax.legend(loc='upper center')\n",
        "    \n",
        "    plt.show()\n",
        "    plt.savefig('Pointwise_LTR_plot.png')\n",
        "\n",
        "    \n",
        "# def train_batch(x_batch, y_batch, model):\n",
        "#     model.net.train()\n",
        "#     x_batch = x_batch.to(device)\n",
        "#     y_batch = y_batch.to(device)\n",
        "           \n",
        "#     output = model.net(x_batch)\n",
        "#     if output.size() != y_batch.size():\n",
        "#         y_batch = y_batch.view(-1, 1)\n",
        "#     loss = model.criterion(y_batch, output)\n",
        "    \n",
        "#     loss.backward()\n",
        "#     model.optimizer.step()\n",
        "#     model.optimizer.zero_grad() \n",
        "    \n",
        "#     return model\n",
        "    \n",
        "       \n",
        "def hyperparam_search():\n",
        "    # hyper-parameters\n",
        "    epochs = 300\n",
        "    learning_rates = [10**-1, 10**-2, 10**-3, 10**-4]\n",
        "    n_hiddens = [100, 150, 200, 250, 300, 350, 400]\n",
        "    data = target_feature()\n",
        "    \n",
        "    best_ndcg = 0\n",
        "    num_q = list(range(1, data.train.num_queries()))\n",
        "    for learning_rate in learning_rates:\n",
        "        for n_hidden in n_hiddens:\n",
        "        \n",
        "            print(\"\\nTesting learning_rate = {} and n_hidden = {}\".format(learning_rate, n_hidden))\n",
        "            model = Model(data.num_features, n_hidden, n_q = num_q , learning_rate)\n",
        "            \n",
        "            last_ndcg = 0\n",
        "            for epoch in range(epochs):\n",
        "                \n",
        "                model.net.train()\n",
        "                Model_trained = Model(data.num_features, n_hidden, n_q = num_q , learning_rate                      \n",
        "                loss, scores = eval_model(model, data.validation)\n",
        "                \n",
        "                ndcg = scores[\"ndcg\"][0]\n",
        "                print(\"Epoch: {}, ndcg: {}\".format(epoch, ndcg))\n",
        "                            \n",
        "                if ndcg < last_ndcg:\n",
        "                    break\n",
        "                last_ndcg = ndcg\n",
        "                if ndcg > best_ndcg:\n",
        "                    best_ndcg = ndcg\n",
        "                    best_params = {\"learning_rate\": learning_rate, \"n_hidden\": n_hidden, \"epoch\": epoch}            \n",
        "                    print(\"Best parameters:\", best_params)\n",
        "    \n",
        "    return best_params\n",
        "    \n",
        "    \n",
        "def train_best(best_params):\n",
        "    epochs = best_params[\"epoch\"]\n",
        "    n_hidden = best_params[\"n_hidden\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    \n",
        "    #load data\n",
        "    data, train_loader = load_dataset()\n",
        "    model = Model(data.num_features, n_hidden, learning_rate)\n",
        "\n",
        "    losses, ndcgs = [], []\n",
        "    for epoch in range(epochs):\n",
        "        eval_count = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            model = train_batch(x_batch, y_batch, model)\n",
        "            eval_count +=1\n",
        "            if eval_count % 2000 == 0:\n",
        "                loss, scores = eval_model(model, data.validation)\n",
        "                losses.append(loss)\n",
        "                ndcgs.append(scores[\"ndcg\"][0])\n",
        "        print(\"Epoch: {}, ndcg: {}\".format(epoch, scores[\"ndcg\"][0]))\n",
        "        \n",
        "    return ndcgs, losses, model\n",
        "\n",
        "\n",
        "def get_distributions(model):\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    model.net.eval()\n",
        "\n",
        "    val_x = torch.from_numpy(data.validation.feature_matrix).float().to(device)\n",
        "    test_x = torch.from_numpy(data.test.feature_matrix).float().to(device)\n",
        "           \n",
        "    val = model.net(val_x).detach().cpu().numpy().squeeze()\n",
        "    test = model.net(test_x).detach().cpu().numpy().squeeze()\n",
        "    actual = np.concatenate((data.train.label_vector, data.validation.label_vector, data.test.label_vector))\n",
        "    \n",
        "    distributions = {\n",
        "    \"val_mean\": np.mean(val),\n",
        "    \"val_std\": np.std(val),\n",
        "    \"test_mean\": np.mean(test),\n",
        "    \"test_std\": np.std(test),\n",
        "    \"actual_mean\": np.mean(actual),\n",
        "    \"actual_std\": np.std(actual),\n",
        "    }\n",
        "    \n",
        "    return distributions\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #determine best hyper parameters\n",
        "    best_params = hyperparam_search()\n",
        "    #train best model\n",
        "    ndcgs, losses, model = train_best(best_params)\n",
        "    #plot ndcg and loss    \n",
        "    plot_ndcg_loss(losses, ndcgs)\n",
        "    #get distributions of scores\n",
        "    distributions = get_distributions(model)\n",
        "    #performance on test set\n",
        "    data = dataset.get_dataset().get_data_folds()[0]\n",
        "    data.read_data()\n",
        "    loss, scores = eval_model(model, data.test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNyCVQ5akqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Ranknet():\n",
        "  def __init__(self, num_features, hidden_units=10, device=\"cpu\", lr = 0.005, layers = 2, early_stopping = 6, use_early_stop=False): #, encoder, decoder, classes, n_protos, bottleneck_size=40, device='cpu', model_type='simple'\n",
        "    '''\n",
        "    arguments:\n",
        "      num_features = size of input vector (features size)\n",
        "    '''\n",
        "    self.num_features = num_features\n",
        "    self.use_early_stop = use_early_stop\n",
        "    self.lr = lr\n",
        "    assert layers in [1,2,3,4], \"layers need to be 1,2,3 or 4\"\n",
        "    if layers == 2:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, 1)\n",
        "          ).to(device)\n",
        "    elif layers == 1:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, 1)\n",
        "          ).to(device)\n",
        "    elif layers == 3:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, 1)\n",
        "          ).to(device)\n",
        "    elif layers == 4:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), 1)\n",
        "          ).to(device)\n",
        "\n",
        "    self.normal_ndcg = [0 for _ in range(early_stopping)]\n",
        "    self.early_stopping = early_stopping\n",
        "    self.output_sig = nn.Sigmoid()\n",
        "    self.device=device\n",
        "    self.model.to(device)\n",
        "    for mod in self.model:\n",
        "      if isinstance(mod, nn.Linear):\n",
        "        nn.init.normal_(mod.weight, mean=2, std=2)\n",
        "        mod.bias.data.fill_(0)\n",
        "  \n",
        "  def forward(self, input_1, input_2):\n",
        "    '''\n",
        "    Does one forward pass using two inputs\n",
        "    '''\n",
        "    si = self.model(input_1.float().to(self.device))\n",
        "    sj = self.model(input_2.float().to(self.device))#self.output_sig(x)\n",
        "    \n",
        "    return si, sj\n",
        "\n",
        "  def get_feature_vectors_target_train(self, s_i, e_i):\n",
        "    docs = data.train.feature_matrix[s_i:e_i]\n",
        "    document_list = list(range(len(docs)))\n",
        "\n",
        "    # create dict from idx of list location to the feature vector\n",
        "    # idx_2_fm = {}\n",
        "    # for e, d in enumerate(docs):\n",
        "    #   idx_2_fm[e] = d\n",
        "\n",
        "    # create all the possible combinations\n",
        "    doc_combs_in_qid = list(itertools.combinations(document_list,2))\n",
        "    input_1, input_2, target = [], [], []\n",
        "  \n",
        "    # iterate over all possible combinations\n",
        "    for i,j in doc_combs_in_qid:\n",
        "      input_1.append(docs[i])\n",
        "      input_2.append(docs[j])\n",
        "      if data.train.label_vector[i+s_i]>data.train.label_vector[j+s_i]:\n",
        "        # this is the S_{ij}\n",
        "        target.append(float(1))\n",
        "      elif data.train.label_vector[i+s_i]<data.train.label_vector[j+s_i]:\n",
        "        target.append(float(-1))\n",
        "      else:\n",
        "        target.append(float(0))\n",
        "    return torch.FloatTensor(input_1), torch.FloatTensor(input_2), torch.FloatTensor(target)\n",
        "\n",
        "  def train(self, n_epochs):\n",
        "    lr=self.lr\n",
        "    if not os.path.exists('loss_folder'):\n",
        "        os.makedirs('loss_folder')\n",
        "\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "    losses = {'Ranking_class': [] }\n",
        "    accuracies = {'Test': [], 'Train': [] }\n",
        "    # data arguments: ['datafold', 'doc_feat', 'doc_str', 'doclist_ranges', 'feature_matrix', 'label_vector', 'name', 'num_docs', 'num_queries', 'query_feat', 'query_labels', 'query_range', 'query_size', 'query_sizes']\n",
        "    for epoch in range(n_epochs):\n",
        "        c_loss = []\n",
        "        train_acc = []\n",
        "        \n",
        "        num_queries = list(range(1, data.train.num_queries()))\n",
        "        # if epoch != 0:\n",
        "        #   random.shuffle(num_queries)\n",
        "        # num_queries = num_queries[0 : int(len(num_queries) * early_stop)]\n",
        "        for qid in num_queries:\n",
        "            s_i, e_i = data.train.query_range(qid)\n",
        "\n",
        "            input_1, input_2, target = self.get_feature_vectors_target_train(s_i, e_i)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "              si, sj = self.forward(input_1, input_2)\n",
        "            except:\n",
        "              print(\"Query with one document, skipping this query\", qid)\n",
        "              continue\n",
        "            # GET PAIRS\n",
        "            loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-target)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
        "\n",
        "            loss.sum().backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if not qid%100:\n",
        "              print(f\"Data: {qid}, with a loss of {loss.sum()}\")\n",
        "              all_scores = self.model(torch.FloatTensor(data.validation.feature_matrix)).view(-1).detach().numpy()\n",
        "              metrics = self.evaluate(data.validation, all_scores)\n",
        "              ndcg = metrics['ndcg']\n",
        "              arr = metrics['relevant rank']\n",
        "              if self.use_early_stop:\n",
        "                self.normal_ndcg.append(ndcg[0])\n",
        "                self.normal_ndcg.pop(0)\n",
        "                x = list(range(0, len(self.normal_ndcg)))\n",
        "                slope, *_ = stats.linregress(x, self.normal_ndcg)\n",
        "                if slope < 0:\n",
        "                  print(f\"negative slope, early stopping activated slope of: {slope}\")\n",
        "                  break\n",
        "                \n",
        "              print(f\"the ndcg is {ndcg} the arr is {arr}\")\n",
        "\n",
        "    all_scores = model.model(torch.FloatTensor(data.train.feature_matrix)).view(-1).detach().numpy()\n",
        "    metrics = model.evaluate(data.train, all_scores)\n",
        "    ndcg = metrics['ndcg']\n",
        "    print(f\"Train ndcg is {ndcg}\")\n",
        "\n",
        "    # create_log(losses, accuracies, log_file)\n",
        "    # torch.save(model.state_dict(), os.path.join(model_folder, 'model_final.pt'))\n",
        "\n",
        "  def predict(self, data_split):\n",
        "      # self.eval()\n",
        "      allscores = []\n",
        "      x_val_tensor = torch.from_numpy(data_split.feature_matrix).float()\n",
        "      y_val_tensor = torch.from_numpy(data_split.label_vector)\n",
        "      \n",
        "      count = 0 \n",
        "      for val, label in zip(x_val_tensor, y_val_tensor):\n",
        "          y_pred = self.model(val)\n",
        "          allscores.append(y_pred.numpy())\n",
        "          \n",
        "      return np.array(allscores)\n",
        "\n",
        "  def evaluate(self, data_split, all_scores):\n",
        "    '''\n",
        "    function to evaluate the model\n",
        "    '''\n",
        "    metrics = evl.evaluate(data_split, all_scores, False)\n",
        "    return metrics\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4jxUhn8akqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "sped_ndcg, sped_arr = [], []\n",
        "class SpedUpRanknet():\n",
        "  def __init__(self, num_features, hidden_units=10, device=\"cpu\", lr = 0.05, layers = 1,  early_stopping=15): #, encoder, decoder, classes, n_protos, bottleneck_size=40, device='cpu', model_type='simple'\n",
        "    '''\n",
        "    arguments:\n",
        "      num_features = size of input vector (features size)\n",
        "    '''\n",
        "    self.normal_ndcg = = [0 for _ in range(early_stopping)]\n",
        "    self.num_features = num_features\n",
        "    self.lr = lr\n",
        "    assert layers in [1,2,3,4], \"layers need to be 1,2,3 or 4\"\n",
        "    if layers == 2:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, 1)\n",
        "          )\n",
        "    elif layers == 1:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, 1)\n",
        "          )\n",
        "    elif layers == 3:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, 1)\n",
        "          )\n",
        "    elif layers == 4:\n",
        "      self.model = nn.Sequential(\n",
        "              nn.Linear(num_features, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), hidden_units),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(hidden_units),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(hidden_units, int(hidden_units/2)),\n",
        "              nn.Dropout(0.5),\n",
        "              nn.BatchNorm1d(int(hidden_units/2)),\n",
        "              nn.LeakyReLU(0.2, inplace=True),\n",
        "              nn.Linear(int(hidden_units/2), 1)\n",
        "          )\n",
        "\n",
        "\n",
        "    self.output_sig = nn.Sigmoid()\n",
        "    self.device=device\n",
        "    self.model.to(device)\n",
        "    for mod in self.model:\n",
        "      if isinstance(mod, nn.Linear):\n",
        "        nn.init.normal_(mod.weight, mean=2, std=2)\n",
        "        mod.bias.data.fill_(0)\n",
        "  \n",
        "  def forward(self, input_1):\n",
        "    '''\n",
        "    Does one forward pass using two inputs\n",
        "    '''\n",
        "    si = self.model(input_1.float())\n",
        "    # sj = self.model(input_2.float())#self.output_sig(x)\n",
        "    \n",
        "    return si\n",
        "\n",
        "  # def get_feature_vectors_target_train(self, s_i, e_i):\n",
        "  #   docs = data.train.feature_matrix[s_i:e_i]\n",
        "  #   document_list = list(range(len(docs)))\n",
        "\n",
        "  #   # create dict from idx of list location to the feature vector\n",
        "  #   idx_2_fm = {}\n",
        "  #   for e, d in enumerate(docs):\n",
        "  #     idx_2_fm[e] = d\n",
        "\n",
        "  #   # create all the possible combinations\n",
        "  #   doc_combs_in_qid = list(itertools.combinations(document_list,2))\n",
        "  #   input_1, input_2, target = [], [], []\n",
        "  \n",
        "  #   # iterate over all possible combinations\n",
        "  #   for i,j in doc_combs_in_qid:\n",
        "  #     input_1.append(docs[i])\n",
        "  #     input_2.append(docs[j])\n",
        "  #     if data.train.label_vector[i+s_i]>data.train.label_vector[j+s_i]:\n",
        "  #       # this is the S_{ij}\n",
        "  #       target.append(float(1))\n",
        "  #     elif data.train.label_vector[i+s_i]<data.train.label_vector[j+s_i]:\n",
        "  #       target.append(float(-1))\n",
        "  #     else:\n",
        "  #       target.append(float(0))\n",
        "  #   return torch.FloatTensor(input_1), torch.FloatTensor(input_2), torch.FloatTensor(target)\n",
        "\n",
        "  def fast_loss(self, forward_output, first_doc, last_doc):\n",
        "    # docs = data.train.feature_matrix[s_i:e_i]\n",
        "    document_list = list(range(last_doc-first_doc))\n",
        "\n",
        "    # idx_2_fm = {}\n",
        "    # for e, d in enumerate(docs):\n",
        "    #   idx_2_fm[e] = d\n",
        "\n",
        "    doc_combs_in_qid = list(itertools.combinations(document_list,2))\n",
        "    q_loss = torch.zeros(1) + (float(0)*forward_output[0])\n",
        "    for i,j in doc_combs_in_qid:\n",
        "      s_i = forward_output[i]\n",
        "      s_j = forward_output[j]\n",
        "      if data.train.label_vector[i+first_doc]>data.train.label_vector[j+first_doc]:\n",
        "        # this is the S_{ij}\n",
        "        S_ij = float(1)\n",
        "      elif data.train.label_vector[i+first_doc]<data.train.label_vector[j+first_doc]:\n",
        "        S_ij = float(-1)\n",
        "      else:\n",
        "        continue\n",
        "      loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S_ij)*((s_i-s_j).sigmoid()).view(s_i.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((s_i-s_j).sigmoid().view(s_i.size(0)))))\n",
        "\n",
        "      # loss = (torch.FloatTensor([0.5])*(torch.FloatTensor([1])-S_ij)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+torch.exp((s_i-s_j).sigmoid().view(s_i.size(0)))))).sigmoid()\n",
        "      q_loss += loss\n",
        "    # print(q_loss)\n",
        "    return q_loss\n",
        "\n",
        "  def train(self, n_epochs):\n",
        "    lr=self.lr\n",
        "    if not os.path.exists('loss_folder'):\n",
        "        os.makedirs('loss_folder')\n",
        "\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "    losses = {'Ranking_class': [] }\n",
        "    accuracies = {'Test': [], 'Train': [] }\n",
        "    # data arguments: ['datafold', 'doc_feat', 'doc_str', 'doclist_ranges', 'feature_matrix', 'label_vector', 'name', 'num_docs', 'num_queries', 'query_feat', 'query_labels', 'query_range', 'query_size', 'query_sizes']\n",
        "    for epoch in range(n_epochs):\n",
        "        c_loss = []\n",
        "        train_acc = []\n",
        "        \n",
        "        num_queries = list(range(1, data.train.num_queries()))\n",
        "        # if epoch != 0:\n",
        "        #   random.shuffle(num_queries)\n",
        "        # num_queries = num_queries[0 : int(len(num_queries) * early_stop)]\n",
        "        for qid in num_queries:\n",
        "            s_i, e_i = data.train.query_range(qid)\n",
        "\n",
        "            # input_1, input_2, target = self.get_feature_vectors_target_train(s_i, e_i)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            input_1 = torch.FloatTensor(data.train.feature_matrix[s_i:e_i])\n",
        "            try:\n",
        "              si = self.forward(input_1)\n",
        "            except:\n",
        "              print(\"Query with one document, skipping this query\", qid)\n",
        "              continue\n",
        "\n",
        "            loss = self.fast_loss(si, s_i, e_i)\n",
        "\n",
        "            # loss = torch.FloatTensor([0.5])*(torch.FloatTensor([1])-target)*((si-sj).sigmoid()).view(si.size(0))+torch.log(torch.FloatTensor([1])+torch.exp(-((si-sj).sigmoid().view(si.size(0)))))\n",
        "            # loss = (torch.FloatTensor([0.5])*(torch.FloatTensor([1])-target)-(torch.FloatTensor([1])/(torch.FloatTensor([1])+torch.exp((si-sj).sigmoid().view(si.size(0)))))).sigmoid()\n",
        "\n",
        "            loss.sum().backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if not qid%100:\n",
        "              print(f\"Data: {qid}, with a loss of {loss.sum()}\")\n",
        "              all_scores = self.model(torch.FloatTensor(data.validation.feature_matrix)).view(-1).detach().numpy()\n",
        "              metrics = self.evaluate(data.validation, all_scores)\n",
        "              ndcg = metrics['ndcg']\n",
        "              arr = metrics['relevant rank']\n",
        "              sped_ndcg.append(ndcg)\n",
        "              sped_arr.append(arr)\n",
        "              print(f\"the ndcg is {ndcg} the arr is {arr}\")\n",
        "              self.normal_ndcg.append(ndcg[0])\n",
        "              self.normal_ndcg.pop(0)\n",
        "              x = list(range(0, len(self.normal_ndcg)))\n",
        "              slope, *_ = stats.linregress(x, self.normal_ndcg)\n",
        "              if slope < 0:\n",
        "                print(f\"negative slope, early stopping activated slope of: {slope}\")\n",
        "                break\n",
        "\n",
        "    all_scores = self.model(torch.FloatTensor(data.train.feature_matrix)).view(-1).detach().numpy()\n",
        "    metrics = self.evaluate(data.train, all_scores)\n",
        "    ndcg = metrics['ndcg']\n",
        "    print(f\"Train ndcg is {ndcg}\")\n",
        "\n",
        "    # create_log(losses, accuracies, log_file)\n",
        "    # torch.save(model.state_dict(), os.path.join(model_folder, 'model_final.pt'))\n",
        "\n",
        "  def predict(self, data_split):\n",
        "      # self.eval()\n",
        "      allscores = []\n",
        "      x_val_tensor = torch.from_numpy(data_split.feature_matrix).float()\n",
        "      y_val_tensor = torch.from_numpy(data_split.label_vector)\n",
        "      \n",
        "      count = 0 \n",
        "      for val, label in zip(x_val_tensor, y_val_tensor):\n",
        "          y_pred = self.model(val)\n",
        "          allscores.append(y_pred.numpy())\n",
        "          \n",
        "      return np.array(allscores)\n",
        "\n",
        "  def evaluate(self, data_split, all_scores):\n",
        "    '''\n",
        "    function to evaluate the model\n",
        "    '''\n",
        "    metrics = evl.evaluate(data_split, all_scores, False)\n",
        "    return metrics\n",
        "\n",
        "sped_model = SpedUpRanknet(num_features=data.num_features, hidden_units=10, lr = 0.05, layers = 1, device='cpu')\n",
        "# print(\"hier\")\n",
        "sped_model.train(n_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}